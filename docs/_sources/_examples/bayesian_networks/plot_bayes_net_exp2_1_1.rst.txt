
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples\bayesian_networks\plot_bayes_net_exp2_1_1.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_bayesian_networks_plot_bayes_net_exp2_1_1.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_bayesian_networks_plot_bayes_net_exp2_1_1.py:


Experiment 2: Model Evaluation
===========================================

The aim of this experiment was to remove multiple features from the data set
satisfying the Missing At Random (MAR) assumption and using the remainining 
features to predict its values to emulate an actual imputer with Bayesian
Networks.

The data was removed in proportions: 10%, 30% and 50%.

.. GENERATED FROM PYTHON SOURCE LINES 15-18

-------------------------------------
Libraries import
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 18-60

.. code-block:: default
   :lineno-start: 19


    # Libraries generic
    import numpy as np
    import pandas as pd
    import sklearn
    import seaborn as sns
    import matplotlib.pyplot as plt
    import warnings
    warnings.filterwarnings("ignore")
    import networkx as nx
    import joblib

    # Libraries sklearn
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import KFold
    from sklearn.model_selection import cross_validate
    from sklearn.preprocessing import StandardScaler, KBinsDiscretizer
    from sklearn.model_selection import train_test_split

    # Regressors
    from sklearn.linear_model import LinearRegression
    from sklearn.linear_model import Ridge
    from sklearn.linear_model import BayesianRidge
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import ExtraTreesRegressor
    from sklearn.linear_model import SGDRegressor
    from sklearn.neighbors import KNeighborsRegressor
    from sklearn.neural_network import MLPRegressor
    from xgboost import XGBRegressor

    # Metrics
    from sklearn.metrics import make_scorer
    from sklearn.metrics import mean_squared_error

    # Custom Packages
    from labimputer.utils.load_dataset import remove_data_outliers
    from labimputer.utils.iter_imp import corr_pairs, get_score_statistics, rmse, norm_rmse, rmsle, get_test_scores, nae, get_best_models, get_cvts_delta
    from labimputer.core.bayes_net import BNRegressor, BNImputer, EMImputer
    from labimputer.core.iter_imp import IterativeImputerRegressor, SimpleImputerRegressor
    from labimputer.utils.bayes_net import get_data_statistics, get_simple_data_stats








.. GENERATED FROM PYTHON SOURCE LINES 61-64

-------------------------------------
Data import 
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 64-97

.. code-block:: default
   :lineno-start: 65


    # Set relative data path and set FBC panel list
    path_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'

    # Define FBC panel for the experiment
    FBC_CODES = sorted(["EOS", "MONO", "BASO", "NEUT", "RBC", "WBC", 
                    "MCHC", "MCV", "LY", "HCT", "RDW", "HGB", 
                    "MCH", "PLT", "MPV", "NRBCA"])

    RBC_ANALYTES = ['HCT', 'HGB', 'RBC', 'MCH', 'MCV', 'MCHC', 'RDW']
    WBC_ANALYTES = ['EOS', 'MONO', 'LY', 'NEUT', 'WBC']
    PLT_ANALYTES = ['PLT', 'MPV']

    # Read data and drop Nan _uid records
    df = pd.read_csv(path_data).dropna(subset=['pid'])

    # Reset the index to easily count all test records
    df.reset_index(drop=True, inplace=True)

    # Obtain the biomarkers DataFrame only
    raw_data = df[FBC_CODES].dropna(subset=FBC_CODES)

    # Remove outliers from dataset
    complete_profiles, _ = remove_data_outliers(raw_data)

    # Constant variables to drop
    DROP_FEATURES = ['BASO', 'NRBCA']

    # Complete profiles for complete case analysis
    complete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)

    FBC_PANEL = complete_profiles.columns








.. GENERATED FROM PYTHON SOURCE LINES 98-101

-------------------------------------
Define tuned estimators
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 101-108

.. code-block:: default
   :lineno-start: 101

    _TUNED_ESTIMATORS = {
        'median': SimpleImputerRegressor(
            strategy='median'
        ),
        'BN': BNRegressor(FBC_PANEL)
    }








.. GENERATED FROM PYTHON SOURCE LINES 109-112

-------------------------------------
Correlation matrix
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 112-122

.. code-block:: default
   :lineno-start: 113


    # Calculate correlation matrix using Pearson Correlation Coefficient
    corr_mat = complete_profiles.corr(method='pearson')

    # Show
    print("\nData:")
    print(complete_profiles)
    print("\nCorrelation (pearson):")
    print(corr_mat)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Data:
            EOS   HCT    HGB   LY   MCH   MCHC    MCV  MONO   MPV  NEUT    PLT   RBC   RDW   WBC
    0       0.0  0.39  125.0  0.7  29.6  320.0   92.6   0.4   8.6   3.9  202.0  4.23  15.1   5.0
    6       0.0  0.37  113.0  1.1  32.0  307.0  104.0   0.7   7.4   4.0  257.0  3.53  15.1   5.9
    7       0.1  0.34  110.0  0.8  32.2  320.0  101.0   0.3   8.0   3.0  282.0  3.40  13.8   4.2
    8       0.1  0.34  108.0  0.7  32.5  321.0  101.0   0.3   8.1   3.4  282.0  3.32  14.4   4.5
    9       0.2  0.34  109.0  0.7  32.7  320.0  102.0   0.6   8.7   4.6  298.0  3.34  14.1   6.1
    ...     ...   ...    ...  ...   ...    ...    ...   ...   ...   ...    ...   ...   ...   ...
    101167  0.1  0.42  138.0  2.1  29.8  328.0   90.8   0.4   9.6   4.4  210.0  4.62  11.3   7.1
    101169  0.2  0.38  128.0  2.0  29.5  334.0   88.2   0.4   9.3   4.9  208.0  4.33  12.9   7.6
    101170  0.2  0.42  134.0  2.2  28.7  323.0   88.8   0.4   8.9   4.0  295.0  4.67  13.9   6.8
    101173  0.0  0.37  122.0  2.1  29.0  325.0   89.4   0.6  10.5   4.5  247.0  4.19  11.1   7.2
    101174  0.0  0.37  121.0  1.2  30.1  326.0   92.4   0.8   9.2   8.1  204.0  4.01  11.7  10.1

    [56271 rows x 14 columns]

    Correlation (pearson):
               EOS       HCT       HGB        LY       MCH  ...      NEUT       PLT       RBC       RDW       WBC
    EOS   1.000000  0.075691  0.063382  0.289178 -0.046216  ... -0.077021  0.148320  0.076247 -0.029017  0.063383
    HCT   0.075691  1.000000  0.983795  0.368346  0.018124  ... -0.147780  0.044596  0.932916 -0.445215 -0.034098
    HGB   0.063382  0.983795  1.000000  0.367455  0.084460  ... -0.142903  0.030526  0.921963 -0.479547 -0.030376
    LY    0.289178  0.368346  0.367455  1.000000 -0.079429  ... -0.067306  0.227641  0.381424 -0.258299  0.242921
    MCH  -0.046216  0.018124  0.084460 -0.079429  1.000000  ... -0.036335 -0.169143 -0.301421 -0.354513 -0.058005
    MCHC -0.073132 -0.009970  0.160611  0.021764  0.391104  ...  0.014154 -0.087255  0.002774 -0.237056  0.015221
    MCV  -0.016068  0.024410  0.016983 -0.097091  0.902952  ... -0.046127 -0.143448 -0.328603 -0.273143 -0.070355
    MONO  0.176592 -0.011767 -0.011822  0.228941 -0.007282  ...  0.437707  0.191892 -0.011525  0.000838  0.565965
    MPV  -0.008346  0.156687  0.148419  0.133033 -0.073755  ... -0.015518 -0.329940  0.169995 -0.129583  0.021949
    NEUT -0.077021 -0.147780 -0.142903 -0.067306 -0.036335  ...  1.000000  0.204307 -0.125506  0.043472  0.946863
    PLT   0.148320  0.044596  0.030526  0.227641 -0.169143  ...  0.204307  1.000000  0.089001 -0.050207  0.275886
    RBC   0.076247  0.932916  0.921963  0.381424 -0.301421  ... -0.125506  0.089001  1.000000 -0.317913 -0.009884
    RDW  -0.029017 -0.445215 -0.479547 -0.258299 -0.354513  ...  0.043472 -0.050207 -0.317913  1.000000 -0.031387
    WBC   0.063383 -0.034098 -0.030376  0.242921 -0.058005  ...  0.946863  0.275886 -0.009884 -0.031387  1.000000

    [14 rows x 14 columns]




.. GENERATED FROM PYTHON SOURCE LINES 123-126

-------------------------------------
Split into train-test
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 126-140

.. code-block:: default
   :lineno-start: 127


    SEED = 8

    # Train-test split of 80:20
    train_set, test_set = train_test_split(complete_profiles, shuffle=False, test_size=0.2, random_state=8)

    # Use copy of the original train and test set
    train_copy, test_copy = train_set.copy(), test_set.copy()

    # Remove 10, 30 or 50% of values depending upon requirements
    for col in train_copy.columns:
        train_copy.loc[train_set.sample(frac=0.1).index, col] = np.nan
        # test_copy.loc[test_set.sample(frac=0.1).index, col] = np.nan








.. GENERATED FROM PYTHON SOURCE LINES 141-144

-------------------------------------
Five fold cross validation (CVTS)
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 144-250

.. code-block:: default
   :lineno-start: 145


    # Number of splits
    n_splits = 5

    # Create Kfold instance
    skf = KFold(n_splits=n_splits, shuffle=False)

    # Scoring
    scoring = {
        'nmae': 'neg_mean_absolute_error', # MAE
        'nmse': 'neg_mean_squared_error',       # MSE
        'nrmse': 'neg_root_mean_squared_error', # RMSE
        'rmsle': make_scorer(rmsle), # RMSLE
        'norm_rmse': make_scorer(norm_rmse), # NRMSE
    }

    # Compendium of results
    bn_results = pd.DataFrame()

    # Create a list of estimators
    ESTIMATORS = [
        # 'median',
        # 'BN',
    ]

    run_eval = False

    if run_eval:

        # Run the EM imputer on training data
        em = EMImputer(max_iter=10, epsilon=0.01)

        # Fit the EM imputer
        em.fit(train_copy)

        # Get transformed data
        Xt_em = em.transform(train_copy)

        # Discretise both training and test set 
        dis = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')

        # Fit on training set to prevent data leakage
        dis.fit(Xt_em)

        # Transform training set
        train_new = pd.DataFrame(dis.transform(Xt_em), columns=FBC_PANEL)

        # Test training set
        test_new = pd.DataFrame(dis.transform(test_set), columns=FBC_PANEL)

        # Initialise imputer
        bn_reg = BNImputer(FBC_PANEL)

        # Collect relevant scores
        test_scores = pd.DataFrame()

        # Loop over each biomarker
        for idx, biomarker in enumerate(FBC_PANEL):

            # Assign training
            auxtrain = train_new.copy()

            # Assign test
            auxtest = test_new.copy()

            # Split training data
            X_train = auxtrain[[x for x in auxtrain.columns if x != biomarker]]
            y_train = auxtrain[biomarker]

            # Set missing values
            for col in FBC_PANEL:
                auxtest.loc[auxtest.sample(frac=0.1).index, col] = np.nan

            # Find index of only missing values
            col_arr = auxtest.to_numpy()[:, idx]

            # Create new missing values
            nan_idx = np.argwhere(np.isnan(col_arr)).flatten()

            # Fit the training data
            bn_reg.fit(X_train, y_train)

            # Only select rows with missing values with that feature
            xtest = auxtest[auxtest[biomarker].isna()]

            # Transform and return predictions
            ypred = pd.DataFrame(bn_reg.transform(auxtest), columns=FBC_PANEL)

            # Get them back in original form
            ypred_hat = dis.inverse_transform(ypred)[:, idx]

            # Flatten to get test array
            ytest = test_set[biomarker].to_numpy().flatten()

            # Create array suitable for data storage
            true_pred_vals = pd.DataFrame(list(zip(ytest, ypred_hat)),
                    columns=[f'{biomarker}-true', f'{biomarker}-pred'])
        
            # Concat the two test score types
            test_scores = pd.concat([test_scores, true_pred_vals], axis=1)

            # Save
            test_scores.to_csv('datasets/bn_mult_test_results_10.csv')










.. GENERATED FROM PYTHON SOURCE LINES 251-254

-------------------------------------
Find and plot data from HOTS
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 254-275

.. code-block:: default
   :lineno-start: 255


    # Read data files
    df1 = pd.read_csv('datasets/bn_mult_test_results_10.csv', index_col=0)
    df2 = pd.read_csv('datasets/bn_mult_test_results_30.csv', index_col=0)
    df3 = pd.read_csv('datasets/bn_mult_test_results_50.csv', index_col=0)
    df4 = pd.read_csv('datasets/bn_simple_test_results.csv', index_col=0)

    # Extract RMSE scores
    hots_10 = get_simple_data_stats(df1, FBC_PANEL, 2)
    hots_30 = get_simple_data_stats(df2, FBC_PANEL, 2)
    hots_50 = get_simple_data_stats(df3, FBC_PANEL, 2)
    median_stats = get_data_statistics(df4, FBC_PANEL, 3)

    median_stats.columns = ['Best', 'Median', 'MW']

    # Concatenate relevant dataframe

    conc = pd.concat([hots_10, hots_30, hots_50, median_stats['Median']], axis=1)

    conc.columns = ['10%', '30%', '50%', 'Median']








.. GENERATED FROM PYTHON SOURCE LINES 276-279

-------------------------------------
RMSE for 10% missing on HOTS
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 279-290

.. code-block:: default
   :lineno-start: 280


    h10 = conc[['10%', 'Median']]

    h10['Delta (%)'] = 100 - (100* (h10['10%']/h10['Median']))

    h10['Model'] = ['BN (10%)' for i in range(h10.shape[0])]

    h10.loc['Mean'] = h10.mean()

    h10






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>10%</th>
          <th>Median</th>
          <th>Delta (%)</th>
          <th>Model</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>EOS</th>
          <td>0.111565</td>
          <td>0.118931</td>
          <td>6.193705</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>HCT</th>
          <td>0.026099</td>
          <td>0.066252</td>
          <td>60.606725</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>HGB</th>
          <td>9.147644</td>
          <td>22.758093</td>
          <td>59.804873</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>LY</th>
          <td>0.615637</td>
          <td>0.738014</td>
          <td>16.581858</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>MCH</th>
          <td>1.063169</td>
          <td>1.916951</td>
          <td>44.538562</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>MCHC</th>
          <td>7.684077</td>
          <td>9.811674</td>
          <td>21.684339</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>MCV</th>
          <td>2.897061</td>
          <td>5.345544</td>
          <td>45.804179</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>MONO</th>
          <td>0.234862</td>
          <td>0.294837</td>
          <td>20.341779</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>MPV</th>
          <td>1.071992</td>
          <td>1.120377</td>
          <td>4.318603</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>NEUT</th>
          <td>1.368072</td>
          <td>2.929796</td>
          <td>53.304857</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>PLT</th>
          <td>67.928586</td>
          <td>76.261661</td>
          <td>10.926951</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>RBC</th>
          <td>0.325174</td>
          <td>0.767018</td>
          <td>57.605407</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>RDW</th>
          <td>1.251922</td>
          <td>1.964734</td>
          <td>36.280345</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>WBC</th>
          <td>1.496463</td>
          <td>2.759500</td>
          <td>45.770498</td>
          <td>BN (10%)</td>
        </tr>
        <tr>
          <th>Mean</th>
          <td>6.801595</td>
          <td>9.060956</td>
          <td>34.554477</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 291-294

-------------------------------------
RMSE for 30% missing on HOTS
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 294-305

.. code-block:: default
   :lineno-start: 295


    h30 = conc[['30%', 'Median']]

    h30['Delta (%)'] = 100 - (100* (h30['30%']/h30['Median']))

    h30['Model'] = ['BN (30%)' for i in range(h30.shape[0])]

    h30.loc['Mean'] = h30.mean()

    h30






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>30%</th>
          <th>Median</th>
          <th>Delta (%)</th>
          <th>Model</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>EOS</th>
          <td>0.116347</td>
          <td>0.118931</td>
          <td>2.172375</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>HCT</th>
          <td>0.032595</td>
          <td>0.066252</td>
          <td>50.800913</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>HGB</th>
          <td>10.683901</td>
          <td>22.758093</td>
          <td>53.054499</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>LY</th>
          <td>0.642812</td>
          <td>0.738014</td>
          <td>12.899674</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>MCH</th>
          <td>1.248165</td>
          <td>1.916951</td>
          <td>34.887994</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>MCHC</th>
          <td>8.544250</td>
          <td>9.811674</td>
          <td>12.917512</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>MCV</th>
          <td>3.753031</td>
          <td>5.345544</td>
          <td>29.791404</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>MONO</th>
          <td>0.226007</td>
          <td>0.294837</td>
          <td>23.345005</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>MPV</th>
          <td>1.091318</td>
          <td>1.120377</td>
          <td>2.593671</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>NEUT</th>
          <td>1.819227</td>
          <td>2.929796</td>
          <td>37.906007</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>PLT</th>
          <td>68.948611</td>
          <td>76.261661</td>
          <td>9.589419</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>RBC</th>
          <td>0.388131</td>
          <td>0.767018</td>
          <td>49.397360</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>RDW</th>
          <td>1.328083</td>
          <td>1.964734</td>
          <td>32.403919</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>WBC</th>
          <td>1.984169</td>
          <td>2.759500</td>
          <td>28.096777</td>
          <td>BN (30%)</td>
        </tr>
        <tr>
          <th>Mean</th>
          <td>7.200475</td>
          <td>9.060956</td>
          <td>27.132609</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 306-309

-------------------------------------
RMSE for 50% missing on HOTS
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 309-320

.. code-block:: default
   :lineno-start: 310


    h50 = conc[['50%', 'Median']]

    h50['Delta (%)'] = 100 - (100* (h50['50%']/h50['Median']))

    h50['Model'] = ['BN (50%)' for i in range(h50.shape[0])]

    h50.loc['Mean'] = h50.mean()

    h50






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>50%</th>
          <th>Median</th>
          <th>Delta (%)</th>
          <th>Model</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>EOS</th>
          <td>0.116885</td>
          <td>0.118931</td>
          <td>1.720596</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>HCT</th>
          <td>0.039826</td>
          <td>0.066252</td>
          <td>39.887385</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>HGB</th>
          <td>11.480353</td>
          <td>22.758093</td>
          <td>49.554854</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>LY</th>
          <td>0.675653</td>
          <td>0.738014</td>
          <td>8.449773</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>MCH</th>
          <td>1.449074</td>
          <td>1.916951</td>
          <td>24.407373</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>MCHC</th>
          <td>9.044579</td>
          <td>9.811674</td>
          <td>7.818189</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>MCV</th>
          <td>3.705048</td>
          <td>5.345544</td>
          <td>30.689038</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>MONO</th>
          <td>0.230790</td>
          <td>0.294837</td>
          <td>21.722753</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>MPV</th>
          <td>1.114487</td>
          <td>1.120377</td>
          <td>0.525697</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>NEUT</th>
          <td>1.879210</td>
          <td>2.929796</td>
          <td>35.858652</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>PLT</th>
          <td>70.493252</td>
          <td>76.261661</td>
          <td>7.563970</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>RBC</th>
          <td>0.449318</td>
          <td>0.767018</td>
          <td>41.420128</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>RDW</th>
          <td>1.341590</td>
          <td>1.964734</td>
          <td>31.716436</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>WBC</th>
          <td>2.100963</td>
          <td>2.759500</td>
          <td>23.864349</td>
          <td>BN (50%)</td>
        </tr>
        <tr>
          <th>Mean</th>
          <td>7.437216</td>
          <td>9.060956</td>
          <td>23.228514</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 321-324

-------------------------------------
Delta for all missing values
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 324-346

.. code-block:: default
   :lineno-start: 325


    # Collect delta and respective model
    pt1 = h10[['Delta (%)', 'Model']]

    pt2 = h30[['Delta (%)', 'Model']]

    pt3 = h50[['Delta (%)', 'Model']]

    comb_df = pd.concat([pt1, pt2, pt3], axis=0)

    # Plot figure
    plt.figure(figsize=(16,6))

    # Plot bar plot
    plot_comb = sns.barplot(x=comb_df.index, y=comb_df['Delta (%)'], hue=comb_df['Model']);

    # Set xlabel
    plot_comb.set_xlabel("Analyte")

    # Show
    plt.show()




.. image:: /_examples/bayesian_networks/images/sphx_glr_plot_bayes_net_exp2_1_1_001.png
    :alt: plot bayes net exp2 1 1
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 347-350

-----------------------------------------
NAE distribution for all missing values
-----------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 350-425

.. code-block:: default
   :lineno-start: 351


    nae_results = pd.DataFrame()

    nae_10 = np.split(df1.T.to_numpy(), len(df1.T.to_numpy())/2)
    nae_30 = np.split(df2.T.to_numpy(), len(df2.T.to_numpy())/2)
    nae_50 = np.split(df3.T.to_numpy(), len(df3.T.to_numpy())/2)
    nae_med = np.split(df4.T.to_numpy(), len(df4.T.to_numpy())/3)

    # Find for 10% panel first 
    for idx, values in enumerate(zip(nae_10, FBC_PANEL)):
    
        y_true, y_pred = values[0][0], values[0][1]
    
        nae_tp = nae(y_true, y_pred)
    
        nae_vals = pd.DataFrame([nae_tp, 
        ['BN (10%)' for _ in range(len(nae_tp))], 
        [values[1] for _ in range(len(nae_tp))]]).T

        nae_results = nae_results.append(nae_vals)

    # Find for 30% panel next
    for idx, values in enumerate(zip(nae_50, FBC_PANEL)):
    
        y_true, y_pred = values[0][0], values[0][1]
    
        nae_tp = nae(y_true, y_pred)
    
        nae_vals = pd.DataFrame([nae_tp, 
        ['BN (30%)' for _ in range(len(nae_tp))], 
        [values[1] for _ in range(len(nae_tp))]]).T

        nae_results = nae_results.append(nae_vals)

    # Find for 50% panel last
    for idx, values in enumerate(zip(nae_50, FBC_PANEL)):

        y_true, y_pred = values[0][0], values[0][1]

        nae_tp = nae(y_true, y_pred)

        nae_vals = pd.DataFrame([nae_tp, 
        ['BN (50%)' for _ in range(len(nae_tp))], 
        [values[1] for _ in range(len(nae_tp))]]).T

        nae_results = nae_results.append(nae_vals)

    # Find for median values 
    for idx, values in enumerate(zip(nae_med, FBC_PANEL)):

        y_true, y_pred, y_med = values[0][0], values[0][1], values[0][2]

        nae_tm = nae(y_true, y_med)

        nae_meds = pd.DataFrame([nae_tm, 
        ['Median' for _ in range(len(nae_tp))], 
        [values[1] for _ in range(len(nae_tp))]]).T

        nae_results = nae_results.append(nae_meds)


    nae_results.columns = ['NAE', 'Model', 'Analyte']

    # Plot
    plt.figure(figsize=(18,6))

    # create grouped boxplot 
    sns.boxplot(x = nae_results['Analyte'],
            y = nae_results['NAE'],
            hue = nae_results['Model'],
            hue_order=['BN (10%)', 'BN (30%)', 'BN (50%)', 'Median'],
            showfliers=False,
            )

    # Show
    plt.show()


.. image:: /_examples/bayesian_networks/images/sphx_glr_plot_bayes_net_exp2_1_1_002.png
    :alt: plot bayes net exp2 1 1
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  24.443 seconds)


.. _sphx_glr_download__examples_bayesian_networks_plot_bayes_net_exp2_1_1.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_bayes_net_exp2_1_1.py <plot_bayes_net_exp2_1_1.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_bayes_net_exp2_1_1.ipynb <plot_bayes_net_exp2_1_1.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
