
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples\iterative_imputer\plot_iter_imputer_exp1_3.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_iterative_imputer_plot_iter_imputer_exp1_3.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_iterative_imputer_plot_iter_imputer_exp1_3.py:


Iterative Imputer Experiment I.III
===========================================

Single biomarker removal using ``sklearn``
methods only.

.. GENERATED FROM PYTHON SOURCE LINES 11-14

-------------------------------------
Libraries import
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 14-48

.. code-block:: default
   :lineno-start: 15


    # Libraries generic
    import numpy as np
    import pandas as pd
    import sklearn
    import matplotlib.pyplot as plt

    # Libraries sklearn
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import KFold
    from sklearn.model_selection import cross_validate
    from sklearn.preprocessing import StandardScaler

    # Regressors
    from sklearn.linear_model import LinearRegression
    from sklearn.linear_model import Ridge
    from sklearn.linear_model import BayesianRidge
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import ExtraTreesRegressor
    from sklearn.linear_model import SGDRegressor
    from sklearn.neighbors import KNeighborsRegressor
    from sklearn.neural_network import MLPRegressor
    from xgboost import XGBRegressor

    # Metrics
    from sklearn.metrics import make_scorer
    from sklearn.metrics import mean_squared_error

    # Custom Packages
    from pkgname.utils.load_dataset import remove_data_outliers
    from pkgname.utils.iter_imp import corr_pairs, get_score_statistics
    from pkgname.core.iter_imp import IterativeImputerRegressor, SimpleImputerRegressor








.. GENERATED FROM PYTHON SOURCE LINES 49-52

-------------------------------------
Define tuned estimators
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 52-110

.. code-block:: default
   :lineno-start: 52

    _TUNED_ESTIMATORS = {
        'lr': LinearRegression(),
        'bridge': BayesianRidge(
            alpha_1=1e-05,
            alpha_2=1e-07,
            lambda_1=1e-07,
            lambda_2=1e-05,
        ),
        'dt': DecisionTreeRegressor(
            criterion='mse',
            splitter='best',
            max_depth=8,
            max_leaf_nodes=15,
            min_samples_leaf=8,
            min_samples_split=8,
        ),
        'etr': ExtraTreesRegressor(
            n_estimators=100,
            criterion='mse',
            bootstrap=False,
            warm_start=False,
            n_jobs=-1,
        ),
        'sgd-ls': SGDRegressor(
            alpha=1e-4,
            epsilon=0.05,
            learning_rate='adaptive',
            loss='squared_loss',
            early_stopping=True,
            warm_start=True,
        ),
        'sgd-sv': SGDRegressor(
            alpha=1e-4,
            epsilon=0.01,
            learning_rate='adaptive',
            loss='squared_epsilon_insensitive',
            early_stopping=True,
            warm_start=True,
        ),
        'knn': KNeighborsRegressor(
            n_neighbors=8,
            weights='distance',
            n_jobs=-1,
        ),
        'xgb': XGBRegressor(),
        'mlp': MLPRegressor(
            alpha=1e-4,
            hidden_layer_sizes=32,
            solver='adam',
            learning_rate='invscaling',
            warm_start=True,
            early_stopping=True,
        ),
        'sir': SimpleImputerRegressor(
            strategy='median'
        ),
    }








.. GENERATED FROM PYTHON SOURCE LINES 111-114

-------------------------------------
Data import 
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 114-139

.. code-block:: default
   :lineno-start: 115


    # Set relative data path and set FBC panel list
    path_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'

    FBC_CODES = ["EOS", "MONO", "BASO", "NEUT", "RBC", "WBC", 
                    "MCHC", "MCV", "LY", "HCT", "RDW", "HGB", 
                    "MCH", "PLT", "MPV", "NRBCA"]

    # Read data and drop Nan _uid records
    df = pd.read_csv(path_data).dropna(subset=['pid'])

    df.reset_index(drop=True, inplace=True)

    # Obtain the biomarkers DataFrame only
    raw_data = df[FBC_CODES].dropna(subset=FBC_CODES)

    # Remove outliers from dataset
    complete_profiles, _ = remove_data_outliers(raw_data)

    # Constant variables to drop
    DROP_FEATURES = ['BASO', 'NRBCA']

    # Complete profiles for complete case analysis
    complete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)








.. GENERATED FROM PYTHON SOURCE LINES 140-143

-------------------------------------
Correlation matrix
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 143-153

.. code-block:: default
   :lineno-start: 144


    # Calculate correlation matrix using Pearson Correlation Coefficient
    corr_mat = complete_profiles.corr(method='pearson')

    # Show
    print("\nData:")
    print(complete_profiles)
    print("\nCorrelation (pearson):")
    print(corr_mat)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Data:
            EOS  MONO  NEUT   RBC   WBC   MCHC    MCV   LY   HCT   RDW    HGB   MCH    PLT   MPV
    0       0.0   0.4   3.9  4.23   5.0  320.0   92.6  0.7  0.39  15.1  125.0  29.6  202.0   8.6
    6       0.0   0.7   4.0  3.53   5.9  307.0  104.0  1.1  0.37  15.1  113.0  32.0  257.0   7.4
    7       0.1   0.3   3.0  3.40   4.2  320.0  101.0  0.8  0.34  13.8  110.0  32.2  282.0   8.0
    8       0.1   0.3   3.4  3.32   4.5  321.0  101.0  0.7  0.34  14.4  108.0  32.5  282.0   8.1
    9       0.2   0.6   4.6  3.34   6.1  320.0  102.0  0.7  0.34  14.1  109.0  32.7  298.0   8.7
    ...     ...   ...   ...   ...   ...    ...    ...  ...   ...   ...    ...   ...    ...   ...
    101167  0.1   0.4   4.4  4.62   7.1  328.0   90.8  2.1  0.42  11.3  138.0  29.8  210.0   9.6
    101169  0.2   0.4   4.9  4.33   7.6  334.0   88.2  2.0  0.38  12.9  128.0  29.5  208.0   9.3
    101170  0.2   0.4   4.0  4.67   6.8  323.0   88.8  2.2  0.42  13.9  134.0  28.7  295.0   8.9
    101173  0.0   0.6   4.5  4.19   7.2  325.0   89.4  2.1  0.37  11.1  122.0  29.0  247.0  10.5
    101174  0.0   0.8   8.1  4.01  10.1  326.0   92.4  1.2  0.37  11.7  121.0  30.1  204.0   9.2

    [56271 rows x 14 columns]

    Correlation (pearson):
               EOS      MONO      NEUT       RBC       WBC  ...       RDW       HGB       MCH       PLT       MPV
    EOS   1.000000  0.176592 -0.077021  0.076247  0.063383  ... -0.029017  0.063382 -0.046216  0.148320 -0.008346
    MONO  0.176592  1.000000  0.437707 -0.011525  0.565965  ...  0.000838 -0.011822 -0.007282  0.191892  0.006449
    NEUT -0.077021  0.437707  1.000000 -0.125506  0.946863  ...  0.043472 -0.142903 -0.036335  0.204307 -0.015518
    RBC   0.076247 -0.011525 -0.125506  1.000000 -0.009884  ... -0.317913  0.921963 -0.301421  0.089001  0.169995
    WBC   0.063383  0.565965  0.946863 -0.009884  1.000000  ... -0.031387 -0.030376 -0.058005  0.275886  0.021949
    MCHC -0.073132 -0.006352  0.014154  0.002774  0.015221  ... -0.237056  0.160611  0.391104 -0.087255 -0.041123
    MCV  -0.016068 -0.005156 -0.046127 -0.328603 -0.070355  ... -0.273143  0.016983  0.902952 -0.143448 -0.061824
    LY    0.289178  0.228941 -0.067306  0.381424  0.242921  ... -0.258299  0.367455 -0.079429  0.227641  0.133033
    HCT   0.075691 -0.011767 -0.147780  0.932916 -0.034098  ... -0.445215  0.983795  0.018124  0.044596  0.156687
    RDW  -0.029017  0.000838  0.043472 -0.317913 -0.031387  ...  1.000000 -0.479547 -0.354513 -0.050207 -0.129583
    HGB   0.063382 -0.011822 -0.142903  0.921963 -0.030376  ... -0.479547  1.000000  0.084460  0.030526  0.148419
    MCH  -0.046216 -0.007282 -0.036335 -0.301421 -0.058005  ... -0.354513  0.084460  1.000000 -0.169143 -0.073755
    PLT   0.148320  0.191892  0.204307  0.089001  0.275886  ... -0.050207  0.030526 -0.169143  1.000000 -0.329940
    MPV  -0.008346  0.006449 -0.015518  0.169995  0.021949  ... -0.129583  0.148419 -0.073755 -0.329940  1.000000

    [14 rows x 14 columns]




.. GENERATED FROM PYTHON SOURCE LINES 154-157

---------------------------------------------
Split complete profiles based on correlations
---------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 157-253

.. code-block:: default
   :lineno-start: 158


    # split MCH-MCHC-MCV from the rest

    SPLIT_FEATURES = ['RBC', 'HCT', 'HGB']
    df1 = complete_profiles[SPLIT_FEATURES] 
    df2 = complete_profiles[[x for x in complete_profiles.columns if x not in SPLIT_FEATURES]]


    # Number of splits
    n_splits = 5

    # Create Kfold instance
    skf = KFold(n_splits=n_splits, shuffle=False)

    # Scoring
    scoring = {
        'nmae': 'neg_mean_absolute_error', # MAE
        'nmse': 'neg_mean_squared_error',       # MSE
        'nrmse': 'neg_root_mean_squared_error', # RMSE
        #'norm_rmse': make_scorer(norm_rmse) # NRMSE
    }

    # Compendium of results
    cb_iir_results = pd.DataFrame()

    # Create a list of estimators
    ESTIMATORS = [
        'lr',
        # 'bridge',
        # 'dt',
        # 'etr',
        # 'sgd-ls',
        # 'sgd-sv',
        # 'knn',
        # 'xgb',
        # 'sir',
    ]

    # Define datasets to obtain scores for
    _TEST_DATA = {
        'cp': complete_profiles,
        'df1': df1,
        'df2': df2,
    }

    # For each estimator
    for i, est in enumerate(ESTIMATORS):

        data = pd.DataFrame()

        # Check if estimator has been defined else skip
        if est not in _TUNED_ESTIMATORS:
            continue
    
        estimator = _TUNED_ESTIMATORS[est]
    
        if estimator != 'sir':
            imputer = IterativeImputerRegressor(estimator=estimator)
        else:
            imputer = estimator

        test_data = _TEST_DATA['df2']

        for biomarker in test_data:

            aux = test_data.copy(deep=True)
            X = aux[[x for x in aux.columns if x != biomarker]]
            y = aux[biomarker]

            # Information
            print("\n%s. Evaluating... %s for biomarker... %s" % (i, est, biomarker))

            # Create pipeline
            pipe = Pipeline(steps=[ ('std', StandardScaler()),
                                    (est, imputer)],
                            verbose=True)

            # Obtain scores for each fold using cross_validate
            scores = cross_validate(pipe, 
                                    X, 
                                    y, 
                                    scoring=scoring, 
                                    cv=skf, 
                                    return_train_score=True, 
                                    n_jobs=-1, 
                                    verbose=0)
        
            # Extract results
            results = pd.DataFrame(scores)
            results.index = ['%s_%s_%s' % (biomarker, est, j)
                for j in range(results.shape[0])]
        
            # Add to compendium
            cb_iir_results = cb_iir_results.append(results)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    0. Evaluating... lr for biomarker... EOS

    0. Evaluating... lr for biomarker... MONO

    0. Evaluating... lr for biomarker... NEUT

    0. Evaluating... lr for biomarker... WBC

    0. Evaluating... lr for biomarker... MCHC

    0. Evaluating... lr for biomarker... MCV

    0. Evaluating... lr for biomarker... LY

    0. Evaluating... lr for biomarker... RDW

    0. Evaluating... lr for biomarker... MCH

    0. Evaluating... lr for biomarker... PLT

    0. Evaluating... lr for biomarker... MPV




.. GENERATED FROM PYTHON SOURCE LINES 254-257

-------------------------------------
Save results
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 257-261

.. code-block:: default
   :lineno-start: 258


    # # Save
    cb_iir_results.to_csv('datasets/cb_iir_results_df2.csv')








.. GENERATED FROM PYTHON SOURCE LINES 262-265

-------------------------------------
Analyse scores and test results
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 265-286

.. code-block:: default
   :lineno-start: 266


    # Combine the two DataFrame together
    combined_df = pd.concat([df1, df2], axis=0)

    # Read results
    compendium_df1 = pd.read_csv('datasets/cb_iir_results_df1.csv', index_col=0)
    compendium_df2 = pd.read_csv('datasets/cb_iir_results_df2.csv', index_col=0)

    # Combine the two compendium together
    combine_compendium = pd.concat([compendium_df1, compendium_df2], axis=0)

    # Obtain RMSE scores
    all_scores = get_score_statistics(combine_compendium, 'rmse')

    # Create DataFrame for mean and std dev statistics
    statistics = pd.DataFrame(all_scores, index=combined_df.columns)

    # Rename the columns
    statistics.columns = ['Mean', 'Std Dev']

    # Show the mean and std dev for linear regressor
    statistics




.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Mean</th>
          <th>Std Dev</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>RBC</th>
          <td>0.266706</td>
          <td>0.006667</td>
        </tr>
        <tr>
          <th>HCT</th>
          <td>0.010782</td>
          <td>0.000234</td>
        </tr>
        <tr>
          <th>HGB</th>
          <td>3.830598</td>
          <td>0.111756</td>
        </tr>
        <tr>
          <th>EOS</th>
          <td>0.058522</td>
          <td>0.000589</td>
        </tr>
        <tr>
          <th>MONO</th>
          <td>0.063986</td>
          <td>0.000266</td>
        </tr>
        <tr>
          <th>NEUT</th>
          <td>0.066184</td>
          <td>0.000360</td>
        </tr>
        <tr>
          <th>WBC</th>
          <td>0.066206</td>
          <td>0.000360</td>
        </tr>
        <tr>
          <th>MCHC</th>
          <td>0.826491</td>
          <td>0.050064</td>
        </tr>
        <tr>
          <th>MCV</th>
          <td>0.234059</td>
          <td>0.016364</td>
        </tr>
        <tr>
          <th>LY</th>
          <td>0.065670</td>
          <td>0.000342</td>
        </tr>
        <tr>
          <th>RDW</th>
          <td>1.592062</td>
          <td>0.101072</td>
        </tr>
        <tr>
          <th>MCH</th>
          <td>0.076054</td>
          <td>0.005021</td>
        </tr>
        <tr>
          <th>PLT</th>
          <td>68.150864</td>
          <td>2.068673</td>
        </tr>
        <tr>
          <th>MPV</th>
          <td>1.011418</td>
          <td>0.017465</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  15.543 seconds)


.. _sphx_glr_download__examples_iterative_imputer_plot_iter_imputer_exp1_3.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_iter_imputer_exp1_3.py <plot_iter_imputer_exp1_3.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_iter_imputer_exp1_3.ipynb <plot_iter_imputer_exp1_3.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
