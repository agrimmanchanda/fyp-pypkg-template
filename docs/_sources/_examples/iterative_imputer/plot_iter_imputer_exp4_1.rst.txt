
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples\iterative_imputer\plot_iter_imputer_exp4_1.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_iterative_imputer_plot_iter_imputer_exp4_1.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_iterative_imputer_plot_iter_imputer_exp4_1.py:


Experiment 4: Model Learning and Evaluation
===========================================

The aim of this experiment was to remove single and multiple features from the data set
satisfying the Missing At Random (MAR) assumption and using the remainining 
features to predict its values to emulate an actual imputer. The data is 
discretised to test the right pre-processing steps.

The data was removed in proportions: 10%, 30% and 50%.

.. GENERATED FROM PYTHON SOURCE LINES 15-18

-------------------------------------
Libraries import
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 18-56

.. code-block:: default
   :lineno-start: 19


    # Libraries generic
    import numpy as np
    import pandas as pd
    import sklearn
    import seaborn as sns
    import matplotlib.pyplot as plt
    import warnings
    warnings.filterwarnings("ignore")

    # Libraries sklearn
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import KFold
    from sklearn.model_selection import cross_validate
    from sklearn.preprocessing import StandardScaler, KBinsDiscretizer
    from sklearn.model_selection import train_test_split

    # Regressors
    from sklearn.linear_model import LinearRegression
    from sklearn.linear_model import Ridge
    from sklearn.linear_model import BayesianRidge
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import ExtraTreesRegressor
    from sklearn.linear_model import SGDRegressor
    from sklearn.neighbors import KNeighborsRegressor
    from sklearn.neural_network import MLPRegressor
    from xgboost import XGBRegressor

    # Metrics
    from sklearn.metrics import make_scorer
    from sklearn.metrics import mean_squared_error

    # Custom Packages
    from labimputer.utils.load_dataset import remove_data_outliers
    from labimputer.utils.iter_imp import corr_pairs, get_score_statistics, rmse, norm_rmse, rmsle, get_test_scores, nae, get_best_models, get_cvts_stats, get_cvts_delta, get_data_statistics, get_simple_data_stats
    from labimputer.core.iter_imp import IterativeImputerRegressor, SimpleImputerRegressor








.. GENERATED FROM PYTHON SOURCE LINES 57-60

-------------------------------------
Define tuned estimators
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 60-111

.. code-block:: default
   :lineno-start: 60

    _TUNED_ESTIMATORS = {
        'lr': LinearRegression(n_jobs=-1),
        'dt': DecisionTreeRegressor(
            criterion='mse',
            splitter='best',
            max_depth=8,
            max_leaf_nodes=15,
            min_samples_leaf=8,
            min_samples_split=8,
        ),
        'rf': ExtraTreesRegressor(
            n_estimators=100,
            criterion='mse',
            max_depth=8,
            bootstrap=False,
            warm_start=False,
            n_jobs=-1,
        ),
        'svr': SGDRegressor(
            alpha=1e-4,
            epsilon=0.05,
            learning_rate='adaptive',
            loss='squared_epsilon_insensitive',
            early_stopping=True,
            warm_start=True,
        ),
        'knn': KNeighborsRegressor(
            n_neighbors=8,
            weights='distance',
            n_jobs=-1,
        ),
        'xgb': XGBRegressor(
            n_estimators=100,
            eval_metric='rmse',
            max_depth=10,
            eta=0.2,
            gamma=0.1,
        ),
        'mlp': MLPRegressor(
            alpha=1e-4,
            hidden_layer_sizes=32,
            solver='adam',
            learning_rate='invscaling',
            warm_start=True,
            early_stopping=True,
        ),
        'median': SimpleImputerRegressor(
            strategy='median'
        ),
    }








.. GENERATED FROM PYTHON SOURCE LINES 112-115

-------------------------------------
Data import 
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 115-144

.. code-block:: default
   :lineno-start: 116


    # Set relative data path and set FBC panel list
    path_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'

    # Define FBC panel for the experiment
    FBC_CODES = sorted(["EOS", "MONO", "BASO", "NEUT", "RBC", "WBC", 
                    "MCHC", "MCV", "LY", "HCT", "RDW", "HGB", 
                    "MCH", "PLT", "MPV", "NRBCA"])

    # Read data and drop Nan _uid records
    df = pd.read_csv(path_data).dropna(subset=['pid'])

    # Reset the index to easily count all test records
    df.reset_index(drop=True, inplace=True)

    # Obtain the biomarkers DataFrame only
    raw_data = df[FBC_CODES].dropna(subset=FBC_CODES)

    # Remove outliers from dataset
    complete_profiles, _ = remove_data_outliers(raw_data)

    # Constant variables to drop
    DROP_FEATURES = ['BASO', 'NRBCA']

    # Complete profiles for complete case analysis
    complete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)

    FBC_PANEL = complete_profiles.columns








.. GENERATED FROM PYTHON SOURCE LINES 145-148

-------------------------------------
Correlation matrix
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 148-158

.. code-block:: default
   :lineno-start: 149


    # Calculate correlation matrix using Pearson Correlation Coefficient
    corr_mat = complete_profiles.corr(method='pearson')

    # Show
    print("\nData:")
    print(complete_profiles)
    print("\nCorrelation (pearson):")
    print(corr_mat)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Data:
            EOS   HCT    HGB   LY   MCH   MCHC    MCV  MONO   MPV  NEUT    PLT   RBC   RDW   WBC
    0       0.0  0.39  125.0  0.7  29.6  320.0   92.6   0.4   8.6   3.9  202.0  4.23  15.1   5.0
    6       0.0  0.37  113.0  1.1  32.0  307.0  104.0   0.7   7.4   4.0  257.0  3.53  15.1   5.9
    7       0.1  0.34  110.0  0.8  32.2  320.0  101.0   0.3   8.0   3.0  282.0  3.40  13.8   4.2
    8       0.1  0.34  108.0  0.7  32.5  321.0  101.0   0.3   8.1   3.4  282.0  3.32  14.4   4.5
    9       0.2  0.34  109.0  0.7  32.7  320.0  102.0   0.6   8.7   4.6  298.0  3.34  14.1   6.1
    ...     ...   ...    ...  ...   ...    ...    ...   ...   ...   ...    ...   ...   ...   ...
    101167  0.1  0.42  138.0  2.1  29.8  328.0   90.8   0.4   9.6   4.4  210.0  4.62  11.3   7.1
    101169  0.2  0.38  128.0  2.0  29.5  334.0   88.2   0.4   9.3   4.9  208.0  4.33  12.9   7.6
    101170  0.2  0.42  134.0  2.2  28.7  323.0   88.8   0.4   8.9   4.0  295.0  4.67  13.9   6.8
    101173  0.0  0.37  122.0  2.1  29.0  325.0   89.4   0.6  10.5   4.5  247.0  4.19  11.1   7.2
    101174  0.0  0.37  121.0  1.2  30.1  326.0   92.4   0.8   9.2   8.1  204.0  4.01  11.7  10.1

    [56271 rows x 14 columns]

    Correlation (pearson):
               EOS       HCT       HGB        LY       MCH      MCHC  ...       MPV      NEUT       PLT       RBC       RDW       WBC
    EOS   1.000000  0.075691  0.063382  0.289178 -0.046216 -0.073132  ... -0.008346 -0.077021  0.148320  0.076247 -0.029017  0.063383
    HCT   0.075691  1.000000  0.983795  0.368346  0.018124 -0.009970  ...  0.156687 -0.147780  0.044596  0.932916 -0.445215 -0.034098
    HGB   0.063382  0.983795  1.000000  0.367455  0.084460  0.160611  ...  0.148419 -0.142903  0.030526  0.921963 -0.479547 -0.030376
    LY    0.289178  0.368346  0.367455  1.000000 -0.079429  0.021764  ...  0.133033 -0.067306  0.227641  0.381424 -0.258299  0.242921
    MCH  -0.046216  0.018124  0.084460 -0.079429  1.000000  0.391104  ... -0.073755 -0.036335 -0.169143 -0.301421 -0.354513 -0.058005
    MCHC -0.073132 -0.009970  0.160611  0.021764  0.391104  1.000000  ... -0.041123  0.014154 -0.087255  0.002774 -0.237056  0.015221
    MCV  -0.016068  0.024410  0.016983 -0.097091  0.902952 -0.040685  ... -0.061824 -0.046127 -0.143448 -0.328603 -0.273143 -0.070355
    MONO  0.176592 -0.011767 -0.011822  0.228941 -0.007282 -0.006352  ...  0.006449  0.437707  0.191892 -0.011525  0.000838  0.565965
    MPV  -0.008346  0.156687  0.148419  0.133033 -0.073755 -0.041123  ...  1.000000 -0.015518 -0.329940  0.169995 -0.129583  0.021949
    NEUT -0.077021 -0.147780 -0.142903 -0.067306 -0.036335  0.014154  ... -0.015518  1.000000  0.204307 -0.125506  0.043472  0.946863
    PLT   0.148320  0.044596  0.030526  0.227641 -0.169143 -0.087255  ... -0.329940  0.204307  1.000000  0.089001 -0.050207  0.275886
    RBC   0.076247  0.932916  0.921963  0.381424 -0.301421  0.002774  ...  0.169995 -0.125506  0.089001  1.000000 -0.317913 -0.009884
    RDW  -0.029017 -0.445215 -0.479547 -0.258299 -0.354513 -0.237056  ... -0.129583  0.043472 -0.050207 -0.317913  1.000000 -0.031387
    WBC   0.063383 -0.034098 -0.030376  0.242921 -0.058005  0.015221  ...  0.021949  0.946863  0.275886 -0.009884 -0.031387  1.000000

    [14 rows x 14 columns]




.. GENERATED FROM PYTHON SOURCE LINES 159-162

-------------------------------------
Split into train-test
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 162-176

.. code-block:: default
   :lineno-start: 163


    SEED = 8

    # Train-test split of 80:20
    train_set, test_set = train_test_split(complete_profiles, shuffle=False, test_size=0.2, random_state=8)

    # Use copy of the original train and test set
    train_copy, test_copy = train_set.copy(), test_set.copy()

    # Remove 10, 30 or 50% of values depending upon requirements
    for col in train_copy.columns:
        train_copy.loc[train_set.sample(frac=0.1).index, col] = np.nan
        test_copy.loc[test_set.sample(frac=0.1).index, col] = np.nan








.. GENERATED FROM PYTHON SOURCE LINES 177-180

-------------------------------------
Five fold cross validation (CVTS)
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 180-291

.. code-block:: default
   :lineno-start: 181


    # Number of splits
    n_splits = 5

    # Create Kfold instance
    skf = KFold(n_splits=n_splits, shuffle=False)

    # Scoring
    scoring = {
        'nmae': 'neg_mean_absolute_error', # MAE
        'nmse': 'neg_mean_squared_error',       # MSE
        'nrmse': 'neg_root_mean_squared_error', # RMSE
        'rmsle': make_scorer(rmsle), # RMSLE
        'norm_rmse': make_scorer(norm_rmse), # NRMSE
    }

    # Compendium of results
    iir_results = pd.DataFrame()

    # Create a list of estimators
    ESTIMATORS = [
        # 'lr',
        # 'dt',
        # 'rf',
        # 'svr',
        # 'knn',
        # 'mlp',
        # 'xgb',
        # 'median',
    ]

    # Concat scores for each CVTS run
    test_data = pd.DataFrame()

    # Loop over each estimator
    for i, est in enumerate(ESTIMATORS):

        # Dictionary for storing all test scores on hold
        test_scores = {}

        # Check if estimator has been defined else skip
        if est not in _TUNED_ESTIMATORS:
            continue
    
        # Select estimator
        estimator = _TUNED_ESTIMATORS[est]
    
        # Select imputer type
        if est != 'median':
            imputer = IterativeImputerRegressor(estimator=estimator,
                                                min_value=0, 
                                                max_iter=10,
                                                verbose=2,
                                                imputation_order='descending')
        else:
            imputer = estimator

        # Loop over each analyte
        for biomarker in train_set:

            # Generate new train-test for each run
            aux_train = train_copy.copy()
            aux_test = test_copy.copy()

            # Define independent (X_train) and dependent (y_train) variables
            X_train = aux_train[[x for x in aux_train.columns if x != biomarker]]
            y_train = aux_train[biomarker]

            # Define same variables with test set
            X_test = aux_test[[x for x in aux_test.columns if x != biomarker]]
            y_test = aux_test[biomarker]

            # Information
            print("\n%s. Evaluating... %s for biomarker... %s" % (i, est, biomarker))

            # Create pipeline
            pipe = Pipeline(steps=[ ('std', StandardScaler()),
                                    ('dis', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform'))
                                    (est, imputer)],
                            verbose=True)

            # Obtain scores for each fold using cross_validate
            scores = cross_validate(pipe, 
                                    X_train, 
                                    y_train, 
                                    scoring=scoring, 
                                    cv=skf, 
                                    return_train_score=True, 
                                    n_jobs=-1, 
                                    verbose=0)

            # Fit on training set 
            pipe.fit(X_train, y_train)

            # Generate x, y test 
            y_pred = pipe.predict(X_test)

            # Compendium of all test scores
            test_scores[biomarker] = get_test_scores(y_test, y_pred)

            # Extract results
            results = pd.DataFrame(scores)
            results.index = ['%s_%s_%s' % (biomarker, est, j)
                for j in range(results.shape[0])]
        
            # Add to compendium of results
            iir_results = iir_results.append(results)
    
        # Concatenate scores for the estimator to all other test scores
        test_data = pd.concat([test_data, pd.Series(test_scores, name=est)], axis=1)








.. GENERATED FROM PYTHON SOURCE LINES 292-295

-------------------------------------
Save results
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 295-300

.. code-block:: default
   :lineno-start: 296


    # Save
    # iir_results.to_csv('datasets/iir_mult_cv_results_10.csv')
    # test_data.to_csv('datasets/iir_mult_test_results_10.csv')








.. GENERATED FROM PYTHON SOURCE LINES 301-304

--------------------------------------------------------
Analysis of simple results from held out test set (HOTS)
--------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 304-320

.. code-block:: default
   :lineno-start: 305


    # Read HOTS results
    hots_single = pd.read_csv('datasets/ML_single_test_results.csv', index_col=0)

    stats_simple_lr = hots_single[:28]
    stat_simple_mlp = hots_single[29:]

    stats_simple_lr = get_simple_data_stats(stats_simple_lr, FBC_PANEL, 2)
    stats_simple_mlp = get_simple_data_stats(stat_simple_mlp, FBC_PANEL, 2)

    join = pd.concat([stats_simple_lr, stats_simple_mlp], axis=1)

    join.columns = ['LR', 'MLP']

    join






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>LR</th>
          <th>MLP</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>EOS</th>
          <td>0.078621</td>
          <td>0.107797</td>
        </tr>
        <tr>
          <th>HCT</th>
          <td>0.017439</td>
          <td>0.017287</td>
        </tr>
        <tr>
          <th>HGB</th>
          <td>5.707196</td>
          <td>5.862825</td>
        </tr>
        <tr>
          <th>LY</th>
          <td>0.549845</td>
          <td>0.561164</td>
        </tr>
        <tr>
          <th>MCH</th>
          <td>0.624110</td>
          <td>0.623721</td>
        </tr>
        <tr>
          <th>MCHC</th>
          <td>5.156447</td>
          <td>6.601652</td>
        </tr>
        <tr>
          <th>MCV</th>
          <td>2.148559</td>
          <td>2.055429</td>
        </tr>
        <tr>
          <th>MONO</th>
          <td>0.238799</td>
          <td>0.201816</td>
        </tr>
        <tr>
          <th>MPV</th>
          <td>1.212124</td>
          <td>0.997807</td>
        </tr>
        <tr>
          <th>NEUT</th>
          <td>0.981907</td>
          <td>0.871244</td>
        </tr>
        <tr>
          <th>PLT</th>
          <td>58.186128</td>
          <td>66.944610</td>
        </tr>
        <tr>
          <th>RBC</th>
          <td>0.180023</td>
          <td>0.214642</td>
        </tr>
        <tr>
          <th>RDW</th>
          <td>1.203382</td>
          <td>1.314592</td>
        </tr>
        <tr>
          <th>WBC</th>
          <td>0.666045</td>
          <td>0.744318</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 321-324

--------------------------------------------------------
Analysis of results from HOTS - 10%
--------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 324-334

.. code-block:: default
   :lineno-start: 325


    # Read HOTS results
    hots_10 = pd.read_csv('datasets/ML_mult_test_results_10.csv', index_col=0)

    stats_10 = get_data_statistics(hots_10, FBC_PANEL, 3)

    stats_10.columns = ['Best', 'Median', 'MWU Test p-value']

    stats_10






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Best</th>
          <th>Median</th>
          <th>MWU Test p-value</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>EOS</th>
          <td>0.112268</td>
          <td>0.124095</td>
          <td>3.742028e-11</td>
        </tr>
        <tr>
          <th>HCT</th>
          <td>0.018242</td>
          <td>0.064654</td>
          <td>2.766324e-01</td>
        </tr>
        <tr>
          <th>HGB</th>
          <td>5.938840</td>
          <td>22.090139</td>
          <td>2.168695e-01</td>
        </tr>
        <tr>
          <th>LY</th>
          <td>0.551620</td>
          <td>0.740583</td>
          <td>2.546511e-01</td>
        </tr>
        <tr>
          <th>MCH</th>
          <td>0.712882</td>
          <td>1.966505</td>
          <td>4.749991e-01</td>
        </tr>
        <tr>
          <th>MCHC</th>
          <td>7.131755</td>
          <td>9.947955</td>
          <td>1.082293e-03</td>
        </tr>
        <tr>
          <th>MCV</th>
          <td>2.297725</td>
          <td>5.547038</td>
          <td>3.136860e-01</td>
        </tr>
        <tr>
          <th>MONO</th>
          <td>0.202145</td>
          <td>0.260055</td>
          <td>6.493039e-02</td>
        </tr>
        <tr>
          <th>MPV</th>
          <td>1.019654</td>
          <td>1.141973</td>
          <td>6.499993e-02</td>
        </tr>
        <tr>
          <th>NEUT</th>
          <td>1.128271</td>
          <td>2.753667</td>
          <td>4.367994e-01</td>
        </tr>
        <tr>
          <th>PLT</th>
          <td>64.623365</td>
          <td>78.276999</td>
          <td>4.232568e-01</td>
        </tr>
        <tr>
          <th>RBC</th>
          <td>0.222702</td>
          <td>0.756030</td>
          <td>4.149508e-01</td>
        </tr>
        <tr>
          <th>RDW</th>
          <td>1.260196</td>
          <td>1.562442</td>
          <td>1.268758e-30</td>
        </tr>
        <tr>
          <th>WBC</th>
          <td>1.082870</td>
          <td>2.803848</td>
          <td>2.920309e-01</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 335-338

--------------------------------------------------------
Analysis of results from HOTS - 30%
--------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 338-348

.. code-block:: default
   :lineno-start: 339


    # Read HOTS results
    hots_30 = pd.read_csv('datasets/ML_mult_test_results_30.csv', index_col=0)

    stats_30 = get_data_statistics(hots_30, FBC_PANEL, 3)

    stats_30.columns = ['Best', 'Median', 'MWU Test p-value']

    stats_30






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Best</th>
          <th>Median</th>
          <th>MWU Test p-value</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>EOS</th>
          <td>0.109829</td>
          <td>0.115248</td>
          <td>3.688781e-38</td>
        </tr>
        <tr>
          <th>HCT</th>
          <td>0.026748</td>
          <td>0.064548</td>
          <td>1.099041e-04</td>
        </tr>
        <tr>
          <th>HGB</th>
          <td>8.836303</td>
          <td>22.215660</td>
          <td>7.060360e-04</td>
        </tr>
        <tr>
          <th>LY</th>
          <td>0.622429</td>
          <td>0.749162</td>
          <td>1.996226e-01</td>
        </tr>
        <tr>
          <th>MCH</th>
          <td>1.330021</td>
          <td>1.941135</td>
          <td>3.612698e-02</td>
        </tr>
        <tr>
          <th>MCHC</th>
          <td>8.966285</td>
          <td>9.868162</td>
          <td>6.896623e-09</td>
        </tr>
        <tr>
          <th>MCV</th>
          <td>4.100758</td>
          <td>5.412234</td>
          <td>1.724310e-03</td>
        </tr>
        <tr>
          <th>MONO</th>
          <td>0.201629</td>
          <td>0.253477</td>
          <td>3.737731e-07</td>
        </tr>
        <tr>
          <th>MPV</th>
          <td>1.036672</td>
          <td>1.097543</td>
          <td>2.707864e-01</td>
        </tr>
        <tr>
          <th>NEUT</th>
          <td>1.559788</td>
          <td>2.763298</td>
          <td>4.747286e-01</td>
        </tr>
        <tr>
          <th>PLT</th>
          <td>69.167517</td>
          <td>77.878602</td>
          <td>7.587634e-02</td>
        </tr>
        <tr>
          <th>RBC</th>
          <td>0.308719</td>
          <td>0.756922</td>
          <td>2.056813e-04</td>
        </tr>
        <tr>
          <th>RDW</th>
          <td>1.314994</td>
          <td>1.613479</td>
          <td>3.606370e-91</td>
        </tr>
        <tr>
          <th>WBC</th>
          <td>1.533757</td>
          <td>2.809900</td>
          <td>1.493336e-03</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 349-352

--------------------------------------------------------
Analysis of results from HOTS - 50%
--------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 352-362

.. code-block:: default
   :lineno-start: 353


    # Read HOTS results
    hots_50 = pd.read_csv('datasets/ML_mult_test_results_50.csv', index_col=0)

    stats_50 = get_data_statistics(hots_50, FBC_PANEL, 3)

    stats_50.columns = ['Best', 'Median', 'MWU Test p-value']

    stats_50






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Best</th>
          <th>Median</th>
          <th>MWU Test p-value</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>EOS</th>
          <td>0.113668</td>
          <td>0.115239</td>
          <td>5.081214e-121</td>
        </tr>
        <tr>
          <th>HCT</th>
          <td>0.038453</td>
          <td>0.064847</td>
          <td>7.952440e-18</td>
        </tr>
        <tr>
          <th>HGB</th>
          <td>12.598351</td>
          <td>21.962906</td>
          <td>1.195674e-19</td>
        </tr>
        <tr>
          <th>LY</th>
          <td>0.690132</td>
          <td>0.736148</td>
          <td>1.431878e-11</td>
        </tr>
        <tr>
          <th>MCH</th>
          <td>1.961003</td>
          <td>1.920643</td>
          <td>1.696174e-06</td>
        </tr>
        <tr>
          <th>MCHC</th>
          <td>10.792757</td>
          <td>9.909622</td>
          <td>1.154612e-20</td>
        </tr>
        <tr>
          <th>MCV</th>
          <td>6.131368</td>
          <td>5.456008</td>
          <td>2.565767e-12</td>
        </tr>
        <tr>
          <th>MONO</th>
          <td>0.222131</td>
          <td>0.258556</td>
          <td>6.331335e-38</td>
        </tr>
        <tr>
          <th>MPV</th>
          <td>1.102159</td>
          <td>1.133922</td>
          <td>2.413366e-01</td>
        </tr>
        <tr>
          <th>NEUT</th>
          <td>1.938168</td>
          <td>2.761872</td>
          <td>1.091473e-02</td>
        </tr>
        <tr>
          <th>PLT</th>
          <td>73.512158</td>
          <td>78.982080</td>
          <td>2.172790e-01</td>
        </tr>
        <tr>
          <th>RBC</th>
          <td>0.432538</td>
          <td>0.759026</td>
          <td>6.595651e-20</td>
        </tr>
        <tr>
          <th>RDW</th>
          <td>1.362755</td>
          <td>1.556153</td>
          <td>1.012036e-136</td>
        </tr>
        <tr>
          <th>WBC</th>
          <td>1.973221</td>
          <td>2.846100</td>
          <td>3.490569e-09</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.778 seconds)


.. _sphx_glr_download__examples_iterative_imputer_plot_iter_imputer_exp4_1.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_iter_imputer_exp4_1.py <plot_iter_imputer_exp4_1.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_iter_imputer_exp4_1.ipynb <plot_iter_imputer_exp4_1.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
