{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiment 2: Model Evaluation\n\nThe aim of this experiment was to remove multiple features from the data set\nsatisfying the Missing At Random (MAR) assumption and using the remainining \nfeatures to predict its values to emulate an actual imputer.\n\nThe data was removed in proportions: 10%, 30% and 50%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries import\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries generic\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy import stats\n\n# Libraries sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n\n# Regressors\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\n# Custom Packages\nfrom labimputer.utils.load_dataset import remove_data_outliers\nfrom labimputer.utils.iter_imp import corr_pairs, get_score_statistics, rmse, norm_rmse, rmsle, get_test_scores, nae, get_best_models, get_cvts_stats\nfrom labimputer.core.iter_imp import IterativeImputerRegressor, SimpleImputerRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define tuned estimators\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_TUNED_ESTIMATORS = {\n    'lr': LinearRegression(n_jobs=-1),\n    'dt': DecisionTreeRegressor(\n        criterion='mse',\n        splitter='best',\n        max_depth=6,\n        max_leaf_nodes=12,\n        min_samples_leaf=8,\n        min_samples_split=8,\n    ),\n    'rf': ExtraTreesRegressor(\n        n_estimators=10,\n        criterion='mse',\n        max_depth=6,\n        bootstrap=False,\n        warm_start=False,\n        n_jobs=-1,\n    ),\n    'svr': SGDRegressor(\n        alpha=1e-4,\n        epsilon=0.01,\n        learning_rate='adaptive',\n        loss='squared_epsilon_insensitive',\n        early_stopping=True,\n        warm_start=True,\n    ),\n    'knn': KNeighborsRegressor(\n        n_neighbors=7,\n        weights='distance',\n        n_jobs=-1,\n    ),\n    'xgb': XGBRegressor(\n        n_estimators=10,\n        eval_metric='rmse',\n        max_depth=6,\n        eta=0.2,\n        gamma=0.1,\n    ),\n    'mlp': MLPRegressor(\n        alpha=1e-4,\n        hidden_layer_sizes=(32,64),\n        solver='adam',\n        learning_rate='invscaling',\n        warm_start=True,\n        early_stopping=True,\n    ),\n    'median': SimpleImputerRegressor(\n        strategy='median'\n    ),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data import \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set relative data path and set FBC panel list\npath_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'\n\n# Define FBC panel for the experiment\nFBC_CODES = sorted([\"EOS\", \"MONO\", \"BASO\", \"NEUT\", \"RBC\", \"WBC\", \n                \"MCHC\", \"MCV\", \"LY\", \"HCT\", \"RDW\", \"HGB\", \n                \"MCH\", \"PLT\", \"MPV\", \"NRBCA\"])\n\n# Read data and drop Nan _uid records\ndf = pd.read_csv(path_data).dropna(subset=['pid'])\n\n# Reset the index to easily count all test records\ndf.reset_index(drop=True, inplace=True)\n\n# Obtain the biomarkers DataFrame only\nraw_data = df[FBC_CODES].dropna(subset=FBC_CODES)\n\n# Remove outliers from dataset\ncomplete_profiles, _ = remove_data_outliers(raw_data)\n\n# Constant variables to drop\nDROP_FEATURES = ['BASO', 'NRBCA']\n\n# Complete profiles for complete case analysis\ncomplete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)\n\nFBC_PANEL = complete_profiles.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix using Pearson Correlation Coefficient\ncorr_mat = complete_profiles.corr(method='pearson')\n\n# Show\nprint(\"\\nData:\")\nprint(complete_profiles)\nprint(\"\\nCorrelation (pearson):\")\nprint(corr_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split into train-test\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SEED = 8\n\n# Train-test split of 80:20\ntrain_set, test_set = train_test_split(complete_profiles, shuffle=False, test_size=0.2, random_state=8)\n\n# Use copy of the original train and test set\ntrain_copy, test_copy = train_set.copy(), test_set.copy()\n\n# Remove 10, 30 or 50% of values depending upon requirements\nfor col in train_copy.columns:\n    train_copy.loc[train_set.sample(frac=0.1).index, col] = np.nan\n    test_copy.loc[test_set.sample(frac=0.1).index, col] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Five fold cross validation (CVTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of splits\nn_splits = 5\n\n# Create Kfold instance\nskf = KFold(n_splits=n_splits, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtain best RMSE scores from (CVTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define tested methods\nMETHODS = [\n    'LR',\n    'DT',\n    'RF',\n    'SVR',\n    'KNN',\n    'MLP',\n    'XGB',\n    'Median',\n]\n\n# Define FBC panel for the experiment\nFBC_PANEL = sorted([\"EOS\", \"MONO\", \"NEUT\", \"RBC\", \"WBC\", \n                \"MCHC\", \"MCV\", \"LY\", \"HCT\", \"RDW\", \"HGB\", \n                \"MCH\", \"PLT\", \"MPV\"])\n\n\n# Read CVTS results - 10%\ncvts_10 = pd.read_csv('datasets/iir_mult_cv_results_10.csv', index_col=0)\n\n# Mean and std stats\nmean_stats, std_stats = get_cvts_stats(cvts_10, FBC_PANEL)\n\n# Find the best models for 10\nBEST_MODELS_10 = get_best_models(mean_stats)\n\n# Read CVTS results - 30%\ncvts_30 = pd.read_csv('datasets/iir_mult_cv_results_30.csv', index_col=0)\n\n# Mean and std stats\nmean_stats, std_stats = get_cvts_stats(cvts_30, FBC_PANEL)\n\n# Find the best models for 30\nBEST_MODELS_30 = get_best_models(mean_stats)\n\n# Read CVTS results - 50%\ncvts_50 = pd.read_csv('datasets/iir_mult_cv_results_50.csv', index_col=0)\n\n# Mean and std stats\nmean_stats, std_stats = get_cvts_stats(cvts_50, FBC_PANEL)\n\n# Find the best models for 50\nBEST_MODELS_50 = get_best_models(mean_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation on held out test set (HOTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set to false to prevent running during script build\nrun_eval = False\n\nif run_eval:\n\n    # Collect relevant scores\n    test_scores = pd.DataFrame()\n\n    # Loop for each model in best models\n    for biomarker, model in BEST_MODELS_10.items():\n\n        for est in [model, 'median']:\n\n            estimator = _TUNED_ESTIMATORS[est]\n\n            # Select estimator\n            if est != 'median':\n                imputer = IterativeImputerRegressor(estimator=estimator,\n                                                    min_value=0, \n                                                    max_iter=10,\n                                                    verbose=2,\n                                                    imputation_order='descending')\n            else:\n                imputer = estimator\n\n            # Generate new train-test for each run\n            aux_train = train_copy.copy()\n            aux_test = test_copy.copy()\n\n            # Define independent (X_train) and dependent (y_train) variables\n            X_train = aux_train[[x for x in aux_train.columns if x != biomarker]]\n            y_train = aux_train[biomarker]\n\n            # Define same variables with test set\n            X_test = aux_test[[x for x in aux_test.columns if x != biomarker]]\n            y_test = aux_test[biomarker]\n\n            # Information\n            print(\"\\n Evaluating... %s for biomarker... %s\" % (est, biomarker))\n\n            # Create pipeline\n            pipe = Pipeline(steps=[ ('std', StandardScaler()),\n                                    (est, imputer)],\n                            verbose=True)\n\n            # Fit on training set \n            pipe.fit(X_train, y_train)\n\n            # Generate x, y test \n            y_pred = pipe.predict(X_test)\n\n            # Store results in DataFrame\n            if est != 'median':\n                true_pred_vals = pd.DataFrame(list(zip(y_test, y_pred)),\n                columns=[f'{biomarker}-{est}-true', f'{biomarker}-{est}-pred'])\n            else:\n                true_pred_vals = pd.Series(y_pred, name=f'{biomarker}-{est}')\n\n            test_scores = pd.concat([test_scores, true_pred_vals], axis=1)\n\n            test_scores.to_csv('datasets/iir_mult_test_results_10.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RMSE for held out test set (HOTS) - 10%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate simple test results\ndf = pd.read_csv('datasets/iir_mult_test_results_10.csv', index_col=0)\n\n# Split the model and median scores for each analyte\nsplit_data = np.split(df.T.to_numpy(), len(df.T.to_numpy())/3)\n\n# DataFrame for RMSE\ndata = pd.DataFrame()\n\n# DataFrame for NAE\nnae_results = pd.DataFrame()\n\n# Iterate through the predicted and median scores\nfor idx, values in enumerate(zip(split_data, FBC_PANEL)):\n    \n    # Extract the true and predicted values\n    y_true, y_pred, y_med = values[0][0], values[0][1], values[0][2]\n    \n    # Obtain the RMSE scores\n    rmse_tp, rmse_tm = rmse(y_true, y_pred), rmse(y_true, y_med)\n\n    # Obtain NAE scores\n    nae_tp, nae_tm = nae(y_true, y_pred), nae(y_true, y_med)\n    \n    nae_vals = pd.DataFrame([nae_tp, \n    ['Best (10%)' for _ in range(len(nae_tp))], \n    [values[1] for _ in range(len(nae_tp))]]).T\n    \n    nae_vals_med = pd.DataFrame([nae_tm, \n    ['Median' for _ in range(len(nae_tm))], \n    [values[1] for _ in range(len(nae_tm))]]).T\n    \n    # Join the RMSE results\n    join_rmse = pd.concat([pd.Series(rmse_tp), pd.Series(rmse_tm)], axis=1)\n\n    # Join the NAE results\n    join_nae = pd.concat([nae_vals, nae_vals_med], axis=0)\n    \n    # Append\n    data = data.append(join_rmse)\n    nae_results = nae_results.append(join_nae)\n\n\n# Create column names and set index\ndata.columns, data.index = ['Best', 'Median'], FBC_PANEL\n\n# Define delta column\ndata['Delta (%)'] = 100 - (100* (data['Best']/data['Median']))\n\n# Set model type\ndata['Model'] = ['Best (10%)' for i in range(data.shape[0])] \n\n# Get mean scores for each model\ndata.loc['Mean'] = data.mean()\n\ndata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RMSE for HOTS - 30%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate simple test results\ndf1 = pd.read_csv('datasets/iir_mult_test_results_30.csv', index_col=0)\n\n# Split the model and median scores for each analyte\nsplit_data1 = np.split(df1.T.to_numpy(), len(df1.T.to_numpy())/3)\n\n# DataFrame for data\ndata1 = pd.DataFrame()\n\n# Iterate through the predicted and median scores\nfor idx, values in enumerate(zip(split_data1, FBC_PANEL)):\n    \n    # Extract the true and predicted values\n    y_true, y_pred, y_med = values[0][0], values[0][1], values[0][2]\n    \n    # Obtain the RMSE scores\n    rmse_tp, rmse_tm = rmse(y_true, y_pred), rmse(y_true, y_med)\n\n    # Obtain NAE scores\n    nae_tp, nae_tm = nae(y_true, y_pred), nae(y_true, y_med)\n    \n    nae_vals = pd.DataFrame([nae_tp, \n    ['Best (30%)' for _ in range(len(nae_tp))], \n    [values[1] for _ in range(len(nae_tp))]]).T\n    \n    nae_vals_med = pd.DataFrame([nae_tm, \n    ['Median' for _ in range(len(nae_tm))], \n    [values[1] for _ in range(len(nae_tm))]]).T\n    \n    # Join the RMSE results\n    join_rmse = pd.concat([pd.Series(rmse_tp), pd.Series(rmse_tm)], axis=1)\n\n    # Join the NAE results\n    join_nae = pd.concat([nae_vals, nae_vals_med], axis=0)\n    \n    # Append\n    data1 = data1.append(join_rmse)\n    nae_results = nae_results.append(join_nae)\n\n# Create column names and set index\ndata1.columns, data1.index = ['Best', 'Median'], FBC_PANEL\n\n# Define delta column\ndata1['Delta (%)'] = 100 - (100* (data1['Best']/data1['Median']))\n\n# Set model type\ndata1['Model'] = ['Best (30%)' for i in range(data1.shape[0])] \n\n# Get mean scores for each model\ndata1.loc['Mean'] = data1.mean()\n\ndata1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RMSE for for HOTS - 50%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate simple test results\ndf2 = pd.read_csv('datasets/iir_mult_test_results_50.csv', index_col=0)\n\n# Split the model and median scores for each analyte\nsplit_data2 = np.split(df2.T.to_numpy(), len(df2.T.to_numpy())/3)\n\n# DataFrame for data\ndata2 = pd.DataFrame()\n\n# Iterate through the predicted and median scores\nfor idx, values in enumerate(zip(split_data2, FBC_PANEL)):\n    \n    # Extract the true and predicted values\n    y_true, y_pred, y_med = values[0][0], values[0][1], values[0][2]\n    \n    # Obtain the RMSE scores\n    rmse_tp, rmse_tm = rmse(y_true, y_pred), rmse(y_true, y_med)\n\n    # Obtain NAE scores\n    nae_tp, nae_tm = nae(y_true, y_pred), nae(y_true, y_med)\n    \n    nae_vals = pd.DataFrame([nae_tp, \n    ['Best (50%)' for _ in range(len(nae_tp))], \n    [values[1] for _ in range(len(nae_tp))]]).T\n    \n    nae_vals_med = pd.DataFrame([nae_tm, \n    ['Median' for _ in range(len(nae_tm))], \n    [values[1] for _ in range(len(nae_tm))]]).T\n    \n    # Join the RMSE results\n    join_rmse = pd.concat([pd.Series(rmse_tp), pd.Series(rmse_tm)], axis=1)\n\n    # Join the NAE results\n    join_nae = pd.concat([nae_vals, nae_vals_med], axis=0)\n    \n    # Append\n    data2 = data2.append(join_rmse)\n    nae_results = nae_results.append(join_nae)\n\n# Create column names and set index\ndata2.columns, data2.index = ['Best', 'Median'], FBC_PANEL\n\n# Define delta column\ndata2['Delta (%)'] = 100 - (100 * (data2['Best']/data2['Median']))\n\n# Set model type\ndata2['Model'] = ['Best (50%)' for i in range(data2.shape[0])] \n\n# Get mean scores for each model\ndata2.loc['Mean'] = data2.mean()\n\ndata2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of Delta to Simple Median Imputation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Select all Delta and Model part of data\npt1 = data[['Delta (%)', 'Model']][:-1]\npt2 = data1[['Delta (%)', 'Model']][:-1]\npt3 = data2[['Delta (%)', 'Model']][:-1]\n\n# Combined all Delta scores together\ncomb_df = pd.concat([pt1, pt2, pt3], axis=0)\n\n# Figure\nplt.figure(figsize=(16,6))\n\n# Plot combined Delta scores\nplot_comb = sns.barplot(x=comb_df.index, y=comb_df['Delta (%)'], hue=comb_df['Model'])\n\n# Set the x label\nplot_comb.set_xlabel(\"Analyte\")\n\n# Show\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NAE distribution for HOTS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nae_results.columns = ['NAE', 'Model', 'Analyte']\n\n# Plot the figure\nplt.figure(figsize=(20,8))\n\n# Create grouped boxplot \nsns.boxplot(x = nae_results['Analyte'],\n        y = nae_results['NAE'],\n        hue = nae_results['Model'],\n        showfliers=False\n        )\n\n# Show\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}