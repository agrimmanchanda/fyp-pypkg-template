{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiment 2: Model Evaluation\n\nThe aim of this experiment was to remove multiple features from the data set\nsatisfying the Missing At Random (MAR) assumption and using the remainining \nfeatures to predict its values to emulate an actual imputer with Bayesian\nNetworks.\n\nThe data was removed in proportions: 10%, 30% and 50%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries import\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries generic\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport networkx as nx\nimport joblib\n\n# Libraries sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\nfrom sklearn.model_selection import train_test_split\n\n# Regressors\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\n# Custom Packages\nfrom labimputer.utils.load_dataset import remove_data_outliers\nfrom labimputer.utils.iter_imp import corr_pairs, get_score_statistics, rmse, norm_rmse, rmsle, get_test_scores, nae, get_best_models, get_cvts_delta\nfrom labimputer.core.bayes_net import BNRegressor, BNImputer, EMImputer\nfrom labimputer.core.iter_imp import IterativeImputerRegressor, SimpleImputerRegressor\nfrom labimputer.utils.bayes_net import get_data_statistics, get_simple_data_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data import \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set relative data path and set FBC panel list\npath_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'\n\n# Define FBC panel for the experiment\nFBC_CODES = sorted([\"EOS\", \"MONO\", \"BASO\", \"NEUT\", \"RBC\", \"WBC\", \n                \"MCHC\", \"MCV\", \"LY\", \"HCT\", \"RDW\", \"HGB\", \n                \"MCH\", \"PLT\", \"MPV\", \"NRBCA\"])\n\nRBC_ANALYTES = ['HCT', 'HGB', 'RBC', 'MCH', 'MCV', 'MCHC', 'RDW']\nWBC_ANALYTES = ['EOS', 'MONO', 'LY', 'NEUT', 'WBC']\nPLT_ANALYTES = ['PLT', 'MPV']\n\n# Read data and drop Nan _uid records\ndf = pd.read_csv(path_data).dropna(subset=['pid'])\n\n# Reset the index to easily count all test records\ndf.reset_index(drop=True, inplace=True)\n\n# Obtain the biomarkers DataFrame only\nraw_data = df[FBC_CODES].dropna(subset=FBC_CODES)\n\n# Remove outliers from dataset\ncomplete_profiles, _ = remove_data_outliers(raw_data)\n\n# Constant variables to drop\nDROP_FEATURES = ['BASO', 'NRBCA']\n\n# Complete profiles for complete case analysis\ncomplete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)\n\nFBC_PANEL = complete_profiles.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define tuned estimators\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_TUNED_ESTIMATORS = {\n    'median': SimpleImputerRegressor(\n        strategy='median'\n    ),\n    'BN': BNRegressor(FBC_PANEL)\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix using Pearson Correlation Coefficient\ncorr_mat = complete_profiles.corr(method='pearson')\n\n# Show\nprint(\"\\nData:\")\nprint(complete_profiles)\nprint(\"\\nCorrelation (pearson):\")\nprint(corr_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split into train-test\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SEED = 8\n\n# Train-test split of 80:20\ntrain_set, test_set = train_test_split(complete_profiles, shuffle=False, test_size=0.2, random_state=8)\n\n# Use copy of the original train and test set\ntrain_copy, test_copy = train_set.copy(), test_set.copy()\n\n# Remove 10, 30 or 50% of values depending upon requirements\nfor col in train_copy.columns:\n    train_copy.loc[train_set.sample(frac=0.1).index, col] = np.nan\n    # test_copy.loc[test_set.sample(frac=0.1).index, col] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Five fold cross validation (CVTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of splits\nn_splits = 5\n\n# Create Kfold instance\nskf = KFold(n_splits=n_splits, shuffle=False)\n\n# Scoring\nscoring = {\n    'nmae': 'neg_mean_absolute_error', # MAE\n    'nmse': 'neg_mean_squared_error',       # MSE\n    'nrmse': 'neg_root_mean_squared_error', # RMSE\n    'rmsle': make_scorer(rmsle), # RMSLE\n    'norm_rmse': make_scorer(norm_rmse), # NRMSE\n}\n\n# Compendium of results\nbn_results = pd.DataFrame()\n\n# Create a list of estimators\nESTIMATORS = [\n    # 'median',\n    # 'BN',\n]\n\nrun_eval = False\n\nif run_eval:\n\n    # Run the EM imputer on training data\n    em = EMImputer(max_iter=10, epsilon=0.01)\n\n    # Fit the EM imputer\n    em.fit(train_copy)\n\n    # Get transformed data\n    Xt_em = em.transform(train_copy)\n\n    # Discretise both training and test set \n    dis = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n\n    # Fit on training set to prevent data leakage\n    dis.fit(Xt_em)\n\n    # Transform training set\n    train_new = pd.DataFrame(dis.transform(Xt_em), columns=FBC_PANEL)\n\n    # Test training set\n    test_new = pd.DataFrame(dis.transform(test_set), columns=FBC_PANEL)\n\n    # Initialise imputer\n    bn_reg = BNImputer(FBC_PANEL)\n\n    # Collect relevant scores\n    test_scores = pd.DataFrame()\n\n    # Loop over each biomarker\n    for idx, biomarker in enumerate(FBC_PANEL):\n\n        # Assign training\n        auxtrain = train_new.copy()\n\n        # Assign test\n        auxtest = test_new.copy()\n\n        # Split training data\n        X_train = auxtrain[[x for x in auxtrain.columns if x != biomarker]]\n        y_train = auxtrain[biomarker]\n\n        # Set missing values\n        for col in FBC_PANEL:\n            auxtest.loc[auxtest.sample(frac=0.1).index, col] = np.nan\n\n        # Find index of only missing values\n        col_arr = auxtest.to_numpy()[:, idx]\n\n        # Create new missing values\n        nan_idx = np.argwhere(np.isnan(col_arr)).flatten()\n\n        # Fit the training data\n        bn_reg.fit(X_train, y_train)\n\n        # Only select rows with missing values with that feature\n        xtest = auxtest[auxtest[biomarker].isna()]\n\n        # Transform and return predictions\n        ypred = pd.DataFrame(bn_reg.transform(auxtest), columns=FBC_PANEL)\n\n        # Get them back in original form\n        ypred_hat = dis.inverse_transform(ypred)[:, idx]\n\n        # Flatten to get test array\n        ytest = test_set[biomarker].to_numpy().flatten()\n\n        # Create array suitable for data storage\n        true_pred_vals = pd.DataFrame(list(zip(ytest, ypred_hat)),\n                columns=[f'{biomarker}-true', f'{biomarker}-pred'])\n        \n        # Concat the two test score types\n        test_scores = pd.concat([test_scores, true_pred_vals], axis=1)\n\n        # Save\n        test_scores.to_csv('datasets/bn_mult_test_results_10.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find and plot data from HOTS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read data files\ndf1 = pd.read_csv('datasets/bn_mult_test_results_10.csv', index_col=0)\ndf2 = pd.read_csv('datasets/bn_mult_test_results_30.csv', index_col=0)\ndf3 = pd.read_csv('datasets/bn_mult_test_results_50.csv', index_col=0)\ndf4 = pd.read_csv('datasets/bn_simple_test_results.csv', index_col=0)\n\n# Extract RMSE scores\nhots_10 = get_simple_data_stats(df1, FBC_PANEL, 2)\nhots_30 = get_simple_data_stats(df2, FBC_PANEL, 2)\nhots_50 = get_simple_data_stats(df3, FBC_PANEL, 2)\nmedian_stats = get_data_statistics(df4, FBC_PANEL, 3)\n\nmedian_stats.columns = ['Best', 'Median', 'MW']\n\n# Concatenate relevant dataframe\n\nconc = pd.concat([hots_10, hots_30, hots_50, median_stats['Median']], axis=1)\n\nconc.columns = ['10%', '30%', '50%', 'Median']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RMSE for 10% missing on HOTS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h10 = conc[['10%', 'Median']]\n\nh10['Delta (%)'] = 100 - (100* (h10['10%']/h10['Median']))\n\nh10['Model'] = ['BN (10%)' for i in range(h10.shape[0])]\n\nh10.loc['Mean'] = h10.mean()\n\nh10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RMSE for 30% missing on HOTS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h30 = conc[['30%', 'Median']]\n\nh30['Delta (%)'] = 100 - (100* (h30['30%']/h30['Median']))\n\nh30['Model'] = ['BN (30%)' for i in range(h30.shape[0])]\n\nh30.loc['Mean'] = h30.mean()\n\nh30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RMSE for 50% missing on HOTS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h50 = conc[['50%', 'Median']]\n\nh50['Delta (%)'] = 100 - (100* (h50['50%']/h50['Median']))\n\nh50['Model'] = ['BN (50%)' for i in range(h50.shape[0])]\n\nh50.loc['Mean'] = h50.mean()\n\nh50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delta for all missing values\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Collect delta and respective model\npt1 = h10[['Delta (%)', 'Model']]\n\npt2 = h30[['Delta (%)', 'Model']]\n\npt3 = h50[['Delta (%)', 'Model']]\n\ncomb_df = pd.concat([pt1, pt2, pt3], axis=0)\n\n# Plot figure\nplt.figure(figsize=(16,6))\n\n# Plot bar plot\nplot_comb = sns.barplot(x=comb_df.index, y=comb_df['Delta (%)'], hue=comb_df['Model']);\n\n# Set xlabel\nplot_comb.set_xlabel(\"Analyte\")\n\n# Show\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NAE distribution for all missing values\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nae_results = pd.DataFrame()\n\nnae_10 = np.split(df1.T.to_numpy(), len(df1.T.to_numpy())/2)\nnae_30 = np.split(df2.T.to_numpy(), len(df2.T.to_numpy())/2)\nnae_50 = np.split(df3.T.to_numpy(), len(df3.T.to_numpy())/2)\nnae_med = np.split(df4.T.to_numpy(), len(df4.T.to_numpy())/3)\n\n# Find for 10% panel first \nfor idx, values in enumerate(zip(nae_10, FBC_PANEL)):\n    \n    y_true, y_pred = values[0][0], values[0][1]\n    \n    nae_tp = nae(y_true, y_pred)\n    \n    nae_vals = pd.DataFrame([nae_tp, \n    ['BN (10%)' for _ in range(len(nae_tp))], \n    [values[1] for _ in range(len(nae_tp))]]).T\n\n    nae_results = nae_results.append(nae_vals)\n\n# Find for 30% panel next\nfor idx, values in enumerate(zip(nae_50, FBC_PANEL)):\n    \n    y_true, y_pred = values[0][0], values[0][1]\n    \n    nae_tp = nae(y_true, y_pred)\n    \n    nae_vals = pd.DataFrame([nae_tp, \n    ['BN (30%)' for _ in range(len(nae_tp))], \n    [values[1] for _ in range(len(nae_tp))]]).T\n\n    nae_results = nae_results.append(nae_vals)\n\n# Find for 50% panel last\nfor idx, values in enumerate(zip(nae_50, FBC_PANEL)):\n\n    y_true, y_pred = values[0][0], values[0][1]\n\n    nae_tp = nae(y_true, y_pred)\n\n    nae_vals = pd.DataFrame([nae_tp, \n    ['BN (50%)' for _ in range(len(nae_tp))], \n    [values[1] for _ in range(len(nae_tp))]]).T\n\n    nae_results = nae_results.append(nae_vals)\n\n# Find for median values \nfor idx, values in enumerate(zip(nae_med, FBC_PANEL)):\n\n    y_true, y_pred, y_med = values[0][0], values[0][1], values[0][2]\n\n    nae_tm = nae(y_true, y_med)\n\n    nae_meds = pd.DataFrame([nae_tm, \n    ['Median' for _ in range(len(nae_tp))], \n    [values[1] for _ in range(len(nae_tp))]]).T\n\n    nae_results = nae_results.append(nae_meds)\n\n\nnae_results.columns = ['NAE', 'Model', 'Analyte']\n\n# Plot\nplt.figure(figsize=(18,6))\n\n# create grouped boxplot \nsns.boxplot(x = nae_results['Analyte'],\n        y = nae_results['NAE'],\n        hue = nae_results['Model'],\n        hue_order=['BN (10%)', 'BN (30%)', 'BN (50%)', 'Median'],\n        showfliers=False,\n        )\n\n# Show\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}