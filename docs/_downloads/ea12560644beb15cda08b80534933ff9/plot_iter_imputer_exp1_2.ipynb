{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Iterative Imputer Experiment I.II\n\nHyperparameter tuning using ``sklearn`` \n``GridSearchCV``. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries import\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries generic\nimport numpy as np\nimport pandas as pd\nimport sklearn\n\n# Libraries sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\n\n# Regressors\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\n# Custom Packages\nfrom pkgname.utils.load_dataset import remove_data_outliers\nfrom pkgname.utils.iter_imp import corr_pairs, get_score_statistics\nfrom pkgname.core.iter_imp import IterativeImputerRegressor, SimpleImputerRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define parameter grids\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "param_grid_lr = {}\nparam_grid_ridge = {\n    'ridge__alpha': [x / 10 for x in range(1, 11)],\n}\n\nparam_grid_bridge = {\n    'bridge__alpha_1': [1e-5, 1e-6, 1e-7],\n    'bridge__alpha_2': [1e-5, 1e-6, 1e-7],\n    'bridge__lambda_1': [1e-5, 1e-6, 1e-7],\n    'bridge__lambda_2': [1e-5, 1e-6, 1e-7],\n}\nparam_grid_iir = {\n    'iir__estimator': [\n        BayesianRidge()\n    ]\n}\n\nparam_grid_dt = {\n    'dt__criterion': [\"mse\", \"mae\"],\n    'dt__max_depth': [8, 12],\n    'dt__min_samples_split': [8, 12],\n    'dt__min_samples_leaf': [8, 12],\n    'dt__max_leaf_nodes': [10, 15],\n}\n\nparam_grid_etr = {\n    'etr__n_estimators': [x*10 for x in range (1, 11)],\n    'etr__criterion': [\"mse\", \"mae\"],\n    'etr__max_depth': [8, 12],\n    'etr__min_samples_split': [8, 12],\n    'etr__bootstrap': [False, True],\n    'etr__warm_start': [False, True]\n}\n\nparam_grid_sgd = {\n    'sgd__loss': [\"squared_loss\", \n                \"huber\", \n                \"epsilon_insensitive\",\n                \"squared_epsilon_insensitive\"],\n    'sgd__alpha': [1e-2, 1e-3, 1e-4],\n    'sgd__epsilon': [0.01, 0.05, 0.1],\n    'sgd__learning_rate': [\"optimal\", \"invscaling\", \"adaptive\"],\n    'sgd__early_stopping': [False, True],\n    'sgd__warm_start': [False, True]\n}\n\nparam_grid_knn = {\n    'knn__n_neighbors': [2, 5, 8],\n    'knn__weights': [\"uniform\", \"distance\"],\n}\n\nparam_grid_sir = {\n    'sir__strategy': [\n        'mean',\n        'median'\n    ]\n}\n\nparam_grid_rfr = {\n    'rfr__n_estimators': [10, 50]\n}\n\n_DEFAULT_PARAM_GRIDS = {\n    'lr': param_grid_lr,\n    'ridge': param_grid_ridge,\n    'bridge': param_grid_bridge,\n    'iir': param_grid_iir,\n    'rfr': param_grid_rfr,\n    'dt': param_grid_dt,\n    'etr': param_grid_etr,\n    'sgd': param_grid_sgd,\n    'knn': param_grid_knn,\n    'sir': param_grid_sir,\n}\n\n_DEFAULT_ESTIMATORS = {\n    'lr': LinearRegression(),\n    'ridge': Ridge(),\n    'bridge': BayesianRidge(),\n    'iir': IterativeImputerRegressor(),\n    'rfr': RandomForestRegressor(),\n    'dt': DecisionTreeRegressor(),\n    'etr': ExtraTreesRegressor(),\n    'sgd': SGDRegressor(max_iter=2000),\n    'knn': KNeighborsRegressor(),\n    'sir': SimpleImputerRegressor(),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data import \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set relative data path and set FBC panel list\npath_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'\n\nFBC_CODES = [\"EOS\", \"MONO\", \"BASO\", \"NEUT\", \"RBC\", \"WBC\", \n                \"MCHC\", \"MCV\", \"LY\", \"HCT\", \"RDW\", \"HGB\", \n                \"MCH\", \"PLT\", \"MPV\", \"NRBCA\"]\n\n# Read data and drop Nan _uid records\ndf = pd.read_csv(path_data).dropna(subset=['pid'])\n\ndf.reset_index(drop=True, inplace=True)\n\n# Obtain the biomarkers DataFrame only\nraw_data = df[FBC_CODES].dropna(subset=FBC_CODES)\n\n# Remove outliers from dataset\ncomplete_profiles, _ = remove_data_outliers(raw_data)\n\n# Constant variables to drop\nDROP_FEATURES = ['BASO', 'NRBCA']\n\ncomplete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix using Pearson Correlation Coefficient\ncorr_mat = complete_profiles.corr(method='pearson')\n\n# Show\nprint(\"\\nData:\")\nprint(complete_profiles)\nprint(\"\\nCorrelation (pearson):\")\nprint(corr_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid Search (with just regressor)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of splits\nn_splits = 5\n\n# Create Kfold instance\nskf = KFold(n_splits=n_splits, shuffle=False)\n\n# Scoring\nscoring = {\n    'nmae': 'neg_mean_absolute_error', # MAE\n    'nmse': 'neg_mean_squared_error',       # MSE\n    'nrmse': 'neg_root_mean_squared_error', # RMSE\n    #'norm_rmse': make_scorer(norm_rmse) # NRMSE\n}\n\n# Parameter Grid\nparam_grid = {}\n\n# Compendium of results\ncompendium = pd.DataFrame()\n\n# Create a list of estimators\nESTIMATORS = [\n    # 'lr',\n    #'ridge',\n    #'bridge',\n    # 'iir',\n    # 'dt',\n    # 'etr',\n    # 'sgd',\n    # 'knn',\n    # 'sir',\n]\n\n# For each estimator\nfor i, est in enumerate(ESTIMATORS):\n\n    data = pd.DataFrame()\n\n    # Basic checks\n    if est not in _DEFAULT_ESTIMATORS:\n        continue\n    if est not in _DEFAULT_PARAM_GRIDS:\n        continue\n\n    for biomarker in complete_profiles:\n\n        aux = complete_profiles.copy(deep=True)\n        X = aux[[x for x in aux.columns if x != biomarker]]\n        y = aux[biomarker]\n\n        # Information\n        print(\"\\n%s. Evaluating... %s for biomarker... %s\" % (i, est, biomarker))\n\n        # Create pipeline\n        pipe = Pipeline(steps=[ ('std', StandardScaler()),\n                                (est, _DEFAULT_ESTIMATORS[est])],\n                        verbose=True)\n\n        # Create grid search (another option is RandomSearchCV)\n        grid = GridSearchCV(pipe, param_grid=_DEFAULT_PARAM_GRIDS[est],\n                            cv=skf, scoring=scoring,\n                            return_train_score=True, verbose=0,\n                            refit=False, n_jobs=-1)\n\n        # Fit grid search\n        grid.fit(X, y)\n\n        # Extract results\n        results = pd.DataFrame(grid.cv_results_)\n        results.index = ['%s_%s_%s' % (est, j, biomarker)\n            for j in range(results.shape[0])]\n        \n        # Add to compendium\n        compendium = compendium.append(results)\n        data = data.append(results)\n        data.to_csv(f'datasets/{est}.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show and save\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Show grid search scores\n# print(\"\\n\\nGrid Search result:\")\n# print(compendium.T)\n\n# Save\n# compendium.to_csv('datasets/compendium.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}