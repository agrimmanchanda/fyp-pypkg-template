{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiment 1: Model Evaluation\n\nThe aim of this experiment was to remove a single feature from the data set \nand use the remaining features to predict its values to emulate a simple \nregression model. This script has results from model evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries import\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries generic\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy import stats\n\n# Libraries sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Regressors\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\n# Custom Packages\nfrom labimputer.utils.load_dataset import remove_data_outliers\nfrom labimputer.utils.iter_imp import corr_pairs, get_score_statistics, rmse, norm_rmse, rmsle, get_test_scores, nae, get_best_models\nfrom labimputer.core.iter_imp import IterativeImputerRegressor, SimpleImputerRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define tuned estimators\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_TUNED_ESTIMATORS = {\n    'lr': LinearRegression(n_jobs=-1),\n    'dt': DecisionTreeRegressor(\n        criterion='mse',\n        splitter='best',\n        max_depth=8,\n        max_leaf_nodes=15,\n        min_samples_leaf=8,\n        min_samples_split=8,\n    ),\n    'rf': ExtraTreesRegressor(\n        n_estimators=100,\n        criterion='mse',\n        max_depth=8,\n        bootstrap=False,\n        warm_start=False,\n        n_jobs=-1,\n    ),\n    'svr': SGDRegressor(\n        alpha=1e-4,\n        epsilon=0.05,\n        learning_rate='adaptive',\n        loss='squared_loss',\n        early_stopping=True,\n        warm_start=True,\n    ),\n    'knn': KNeighborsRegressor(\n        n_neighbors=8,\n        weights='distance',\n        n_jobs=-1,\n    ),\n    'xgb': XGBRegressor(\n        n_estimators=100,\n        eval_metric='rmse',\n        max_depth=10,\n        eta=0.2,\n        gamma=0.1,\n    ),\n    'mlp': MLPRegressor(\n        alpha=1e-4,\n        hidden_layer_sizes=32,\n        solver='adam',\n        learning_rate='invscaling',\n        warm_start=True,\n        early_stopping=True,\n    ),\n    'median': SimpleImputerRegressor(\n        strategy='median'\n    ),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data import \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set relative data path and set FBC panel list\npath_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'\n\n# Define FBC panel for the experiment\nFBC_CODES = sorted([\"EOS\", \"MONO\", \"BASO\", \"NEUT\", \"RBC\", \"WBC\", \n                \"MCHC\", \"MCV\", \"LY\", \"HCT\", \"RDW\", \"HGB\", \n                \"MCH\", \"PLT\", \"MPV\", \"NRBCA\"])\n\n# Read data and drop Nan _uid records\ndf = pd.read_csv(path_data).dropna(subset=['pid'])\n\n# Reset the index to easily count all test records\ndf.reset_index(drop=True, inplace=True)\n\n# Obtain the biomarkers DataFrame only\nraw_data = df[FBC_CODES].dropna(subset=FBC_CODES)\n\n# Remove outliers from dataset\ncomplete_profiles, _ = remove_data_outliers(raw_data)\n\n# Constant variables to drop\nDROP_FEATURES = ['BASO', 'NRBCA']\n\n# Complete profiles for complete case analysis\ncomplete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)\n\nFBC_PANEL = complete_profiles.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix using Pearson Correlation Coefficient\ncorr_mat = complete_profiles.corr(method='pearson')\n\n# Show\nprint(\"\\nData:\")\nprint(complete_profiles)\nprint(\"\\nCorrelation (pearson):\")\nprint(corr_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split into train-test\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SEED = 8\n\n# Train-test split of 80:20\ntrain_set, test_set = train_test_split(complete_profiles, shuffle=False, test_size=0.2, random_state=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Five fold cross validation (CVTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of splits\nn_splits = 5\n\n# Create Kfold instance\nskf = KFold(n_splits=n_splits, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtain best RMSE scores from cross validation test set (CVTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define tested methods\nMETHODS = [\n    'LR',\n    'DT',\n    'RF',\n    'SVR',\n    'KNN',\n    'MLP',\n    'XGB',\n    'Median',\n]\n\n# Define FBC panel for the experiment\nFBC_PANEL = sorted([\"EOS\", \"MONO\", \"NEUT\", \"RBC\", \"WBC\", \n                \"MCHC\", \"MCV\", \"LY\", \"HCT\", \"RDW\", \"HGB\", \n                \"MCH\", \"PLT\", \"MPV\"])\n\n# Read CVTS results\ncvts = pd.read_csv('datasets/iir_simple_cv_results.csv', index_col=0)\n\n# Get mean and variance of RMSE scores\nall_scores = get_score_statistics(cvts, metric='rmse')\n\n# Split scores to obtain score for each estimator\nsplit_scores = np.array_split(all_scores, 8)\n\n# Stack scores horizontally for easier plotting\nhsplit_scores = np.hstack((split_scores))\n\n# Create DataFrame for mean and std dev statistics\nstatistics = pd.DataFrame(hsplit_scores, index=FBC_PANEL)\n\n# Split mean and std dev statistics\nmean_stats, std_stats = statistics.iloc[:,::2], statistics.iloc[:,1::2]\n\n# Rename columns to match algorithms\nmean_stats.columns, std_stats.columns = METHODS, METHODS\n\n# Select best models based on best CVTS score for each analyte\nBEST_MODELS = get_best_models(mean_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation on held out test set (HOTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set to false to prevent running during script build\nrun_eval = False\n\nif run_eval:\n\n    # Collect relevant scores\n    test_scores = pd.DataFrame()\n\n    # Loop for each model in best models\n    for biomarker, model in BEST_MODELS.items():\n\n        for est in [model, 'median']:\n\n            estimator = _TUNED_ESTIMATORS[est]\n\n            # Select estimator\n            if est != 'median':\n                imputer = IterativeImputerRegressor(estimator=estimator,\n                                                    min_value=0, \n                                                    max_iter=10,\n                                                    verbose=2,\n                                                    imputation_order='descending')\n            else:\n                imputer = estimator\n\n            # Generate new train-test for each run\n            aux_train = train_set.copy()\n            aux_test = test_set.copy()\n\n            # Define independent (X_train) and dependent (y_train) variables\n            X_train = aux_train[[x for x in aux_train.columns if x != biomarker]]\n            y_train = aux_train[biomarker]\n\n            # Define same variables with test set\n            X_test = aux_test[[x for x in aux_test.columns if x != biomarker]]\n            y_test = aux_test[biomarker]\n\n            # Information\n            print(\"\\n Evaluating... %s for biomarker... %s\" % (est, biomarker))\n\n            # Create pipeline\n            pipe = Pipeline(steps=[ ('std', StandardScaler()),\n                                    (est, imputer)],\n                            verbose=True)\n\n            # Fit on training set \n            pipe.fit(X_train, y_train)\n\n            # Generate x, y test \n            y_pred = pipe.predict(X_test)\n\n            # Store results in DataFrame\n            if est != 'median':\n                true_pred_vals = pd.DataFrame(list(zip(y_test, y_pred)),\n                columns=[f'{biomarker}-{est}-true', f'{biomarker}-{est}-pred'])\n            else:\n                true_pred_vals = pd.Series(y_pred, name=f'{biomarker}-{est}')\n\n            test_scores = pd.concat([test_scores, true_pred_vals], axis=1)\n\n            test_scores.to_csv('datasets/iir_simple_test_results.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RMSE for held out test set (HOTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate simple test results\ndf = pd.read_csv('datasets/iir_simple_test_results.csv', index_col=0)\n\n# Split the model and median scores for each analyte\nsplit_data = np.split(df.T.to_numpy(), len(df.T.to_numpy())/3)\n\n# DataFrame for data\ndata = pd.DataFrame()\n\n# Iterate through the predicted and median scores\nfor idx, values in enumerate(zip(split_data, FBC_PANEL)):\n    \n    # Extract the true and predicted values\n    y_true, y_pred, y_med = values[0][0], values[0][1], values[0][2]\n    \n    # Obtain the RMSE scores\n    rmse_tp, rmse_tm = rmse(y_true, y_pred), rmse(y_true, y_med)\n    \n    # Join the results\n    join = pd.concat([pd.Series(rmse_tp), pd.Series(rmse_tm)], axis=1)\n    \n    # Append\n    data = data.append(join)\n\n# Create column names and set index\ndata.columns, data.index = ['Best', 'Median'], FBC_PANEL\n\n# Define delta column\ndata['Delta (%)'] = 100 - (100* (data['Best']/data['Median']))\n\n# Get the best models from CVTS\ndata['Model'] = BEST_MODELS.values()\n\n# Get mean scores for each model\ndata.loc['Mean'] = data.mean()\n\ndata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mann Whitney U-test for HOTS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create empty DataFrame\nmwu_df = pd.DataFrame()\n\n# Loop through results\nfor idx, values in enumerate(zip(split_data, FBC_PANEL)):\n    \n    # Extract true and median scores\n    y_true, y_pred, y_med = values[0][0], values[0][1], values[0][2]\n    \n    # Carry out MWU test\n    mwu_tp, mwu_tm = stats.mannwhitneyu(y_true, y_pred)[1], stats.mannwhitneyu(y_true, y_med)[1]\n\n    # Join relevant cores\n    join = pd.concat([pd.Series(mwu_tp), pd.Series(mwu_tm)], axis=1)\n    \n    # Append relevant scores\n    mwu_df = mwu_df.append(join)\n\n# Display the p-values for best and median\nmwu_df.columns, mwu_df.index = ['Best: p-value', 'Median: p-value'], FBC_PANEL\n\nmwu_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NAE distribution for HOTS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create new DataFrame\nnae_results = pd.DataFrame()\n\n# Loop through data\nfor idx, values in enumerate(zip(split_data, FBC_PANEL)):\n    \n    # Get the predicted and median scores\n    y_true, y_pred, y_med = values[0][0], values[0][1], values[0][2]\n    \n    # Get the NAE scores for predicted and median\n    nae_tp, nae_tm = nae(y_true, y_pred), nae(y_true, y_med)\n    \n    # Get the best scores\n    nae_vals = pd.DataFrame([nae_tp, \n    ['Best' for _ in range(len(nae_tp))], \n    [values[1] for _ in range(len(nae_tp))]]).T\n    \n    # Get the median scores\n    nae_vals_med = pd.DataFrame([nae_tm, \n    ['Median' for _ in range(len(nae_tm))], \n    [values[1] for _ in range(len(nae_tm))]]).T\n    \n    # Join scores\n    join = pd.concat([nae_vals, nae_vals_med], axis=0)\n    \n    # Append\n    nae_results = nae_results.append(join)\n\n# Set columns\nnae_results.columns = ['NAE', 'Model', 'Analyte']\n\n# Plot the figure\nplt.figure(figsize=(20,8))\n\n# Create grouped boxplot \nsns.boxplot(x = nae_results['Analyte'],\n        y = nae_results['NAE'],\n        hue = nae_results['Model'],\n        showfliers=False\n        )\n\n# Show\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}