{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiment 4: Model Learning and Evaluation\n\nThe aim of this experiment was to remove single and multiple features from the data set\nsatisfying the Missing At Random (MAR) assumption and using the remainining \nfeatures to predict its values to emulate an actual imputer. The data is \ndiscretised to test the right pre-processing steps.\n\nThe data was removed in proportions: 10%, 30% and 50%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries import\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries generic\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Libraries sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\nfrom sklearn.model_selection import train_test_split\n\n# Regressors\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\n# Custom Packages\nfrom labimputer.utils.load_dataset import remove_data_outliers\nfrom labimputer.utils.iter_imp import corr_pairs, get_score_statistics, rmse, norm_rmse, rmsle, get_test_scores, nae, get_best_models, get_cvts_stats, get_cvts_delta, get_data_statistics, get_simple_data_stats\nfrom labimputer.core.iter_imp import IterativeImputerRegressor, SimpleImputerRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define tuned estimators\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_TUNED_ESTIMATORS = {\n    'lr': LinearRegression(n_jobs=-1),\n    'dt': DecisionTreeRegressor(\n        criterion='mse',\n        splitter='best',\n        max_depth=8,\n        max_leaf_nodes=15,\n        min_samples_leaf=8,\n        min_samples_split=8,\n    ),\n    'rf': ExtraTreesRegressor(\n        n_estimators=100,\n        criterion='mse',\n        max_depth=8,\n        bootstrap=False,\n        warm_start=False,\n        n_jobs=-1,\n    ),\n    'svr': SGDRegressor(\n        alpha=1e-4,\n        epsilon=0.05,\n        learning_rate='adaptive',\n        loss='squared_epsilon_insensitive',\n        early_stopping=True,\n        warm_start=True,\n    ),\n    'knn': KNeighborsRegressor(\n        n_neighbors=8,\n        weights='distance',\n        n_jobs=-1,\n    ),\n    'xgb': XGBRegressor(\n        n_estimators=100,\n        eval_metric='rmse',\n        max_depth=10,\n        eta=0.2,\n        gamma=0.1,\n    ),\n    'mlp': MLPRegressor(\n        alpha=1e-4,\n        hidden_layer_sizes=32,\n        solver='adam',\n        learning_rate='invscaling',\n        warm_start=True,\n        early_stopping=True,\n    ),\n    'median': SimpleImputerRegressor(\n        strategy='median'\n    ),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data import \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set relative data path and set FBC panel list\npath_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'\n\n# Define FBC panel for the experiment\nFBC_CODES = sorted([\"EOS\", \"MONO\", \"BASO\", \"NEUT\", \"RBC\", \"WBC\", \n                \"MCHC\", \"MCV\", \"LY\", \"HCT\", \"RDW\", \"HGB\", \n                \"MCH\", \"PLT\", \"MPV\", \"NRBCA\"])\n\n# Read data and drop Nan _uid records\ndf = pd.read_csv(path_data).dropna(subset=['pid'])\n\n# Reset the index to easily count all test records\ndf.reset_index(drop=True, inplace=True)\n\n# Obtain the biomarkers DataFrame only\nraw_data = df[FBC_CODES].dropna(subset=FBC_CODES)\n\n# Remove outliers from dataset\ncomplete_profiles, _ = remove_data_outliers(raw_data)\n\n# Constant variables to drop\nDROP_FEATURES = ['BASO', 'NRBCA']\n\n# Complete profiles for complete case analysis\ncomplete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)\n\nFBC_PANEL = complete_profiles.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix using Pearson Correlation Coefficient\ncorr_mat = complete_profiles.corr(method='pearson')\n\n# Show\nprint(\"\\nData:\")\nprint(complete_profiles)\nprint(\"\\nCorrelation (pearson):\")\nprint(corr_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split into train-test\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SEED = 8\n\n# Train-test split of 80:20\ntrain_set, test_set = train_test_split(complete_profiles, shuffle=False, test_size=0.2, random_state=8)\n\n# Use copy of the original train and test set\ntrain_copy, test_copy = train_set.copy(), test_set.copy()\n\n# Remove 10, 30 or 50% of values depending upon requirements\nfor col in train_copy.columns:\n    train_copy.loc[train_set.sample(frac=0.1).index, col] = np.nan\n    test_copy.loc[test_set.sample(frac=0.1).index, col] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Five fold cross validation (CVTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of splits\nn_splits = 5\n\n# Create Kfold instance\nskf = KFold(n_splits=n_splits, shuffle=False)\n\n# Scoring\nscoring = {\n    'nmae': 'neg_mean_absolute_error', # MAE\n    'nmse': 'neg_mean_squared_error',       # MSE\n    'nrmse': 'neg_root_mean_squared_error', # RMSE\n    'rmsle': make_scorer(rmsle), # RMSLE\n    'norm_rmse': make_scorer(norm_rmse), # NRMSE\n}\n\n# Compendium of results\niir_results = pd.DataFrame()\n\n# Create a list of estimators\nESTIMATORS = [\n    # 'lr',\n    # 'dt',\n    # 'rf',\n    # 'svr',\n    # 'knn',\n    # 'mlp',\n    # 'xgb',\n    # 'median',\n]\n\n# Concat scores for each CVTS run\ntest_data = pd.DataFrame()\n\n# Loop over each estimator\nfor i, est in enumerate(ESTIMATORS):\n\n    # Dictionary for storing all test scores on hold\n    test_scores = {}\n\n    # Check if estimator has been defined else skip\n    if est not in _TUNED_ESTIMATORS:\n        continue\n    \n    # Select estimator\n    estimator = _TUNED_ESTIMATORS[est]\n    \n    # Select imputer type\n    if est != 'median':\n        imputer = IterativeImputerRegressor(estimator=estimator,\n                                            min_value=0, \n                                            max_iter=10,\n                                            verbose=2,\n                                            imputation_order='descending')\n    else:\n        imputer = estimator\n\n    # Loop over each analyte\n    for biomarker in train_set:\n\n        # Generate new train-test for each run\n        aux_train = train_copy.copy()\n        aux_test = test_copy.copy()\n\n        # Define independent (X_train) and dependent (y_train) variables\n        X_train = aux_train[[x for x in aux_train.columns if x != biomarker]]\n        y_train = aux_train[biomarker]\n\n        # Define same variables with test set\n        X_test = aux_test[[x for x in aux_test.columns if x != biomarker]]\n        y_test = aux_test[biomarker]\n\n        # Information\n        print(\"\\n%s. Evaluating... %s for biomarker... %s\" % (i, est, biomarker))\n\n        # Create pipeline\n        pipe = Pipeline(steps=[ ('std', StandardScaler()),\n                                ('dis', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform'))\n                                (est, imputer)],\n                        verbose=True)\n\n        # Obtain scores for each fold using cross_validate\n        scores = cross_validate(pipe, \n                                X_train, \n                                y_train, \n                                scoring=scoring, \n                                cv=skf, \n                                return_train_score=True, \n                                n_jobs=-1, \n                                verbose=0)\n\n        # Fit on training set \n        pipe.fit(X_train, y_train)\n\n        # Generate x, y test \n        y_pred = pipe.predict(X_test)\n\n        # Compendium of all test scores\n        test_scores[biomarker] = get_test_scores(y_test, y_pred)\n\n        # Extract results\n        results = pd.DataFrame(scores)\n        results.index = ['%s_%s_%s' % (biomarker, est, j)\n            for j in range(results.shape[0])]\n        \n        # Add to compendium of results\n        iir_results = iir_results.append(results)\n    \n    # Concatenate scores for the estimator to all other test scores\n    test_data = pd.concat([test_data, pd.Series(test_scores, name=est)], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Save\n# iir_results.to_csv('datasets/iir_mult_cv_results_10.csv')\n# test_data.to_csv('datasets/iir_mult_test_results_10.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of simple results from held out test set (HOTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read HOTS results\nhots_single = pd.read_csv('datasets/ML_single_test_results.csv', index_col=0)\n\nstats_simple_lr = hots_single[:28]\nstat_simple_mlp = hots_single[29:]\n\nstats_simple_lr = get_simple_data_stats(stats_simple_lr, FBC_PANEL, 2)\nstats_simple_mlp = get_simple_data_stats(stat_simple_mlp, FBC_PANEL, 2)\n\njoin = pd.concat([stats_simple_lr, stats_simple_mlp], axis=1)\n\njoin.columns = ['LR', 'MLP']\n\njoin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of results from HOTS - 10%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read HOTS results\nhots_10 = pd.read_csv('datasets/ML_mult_test_results_10.csv', index_col=0)\n\nstats_10 = get_data_statistics(hots_10, FBC_PANEL, 3)\n\nstats_10.columns = ['Best', 'Median', 'MWU Test p-value']\n\nstats_10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of results from HOTS - 30%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read HOTS results\nhots_30 = pd.read_csv('datasets/ML_mult_test_results_30.csv', index_col=0)\n\nstats_30 = get_data_statistics(hots_30, FBC_PANEL, 3)\n\nstats_30.columns = ['Best', 'Median', 'MWU Test p-value']\n\nstats_30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of results from HOTS - 50%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read HOTS results\nhots_50 = pd.read_csv('datasets/ML_mult_test_results_50.csv', index_col=0)\n\nstats_50 = get_data_statistics(hots_50, FBC_PANEL, 3)\n\nstats_50.columns = ['Best', 'Median', 'MWU Test p-value']\n\nstats_50"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}