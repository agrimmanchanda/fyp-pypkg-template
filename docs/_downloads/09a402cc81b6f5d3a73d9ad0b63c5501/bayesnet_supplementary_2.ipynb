{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Supplementary Experiment I.I.I\n\nUsing the ``pgmpy`` library to learn the \nstructure of Bayesian Network (BN) from the data,\nestimate parameters for Conditional Probability \nDistributions (CPDs) and imputing missing values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries import\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries generic\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom math import isnan\n\n# Libraries for BNs\nimport networkx as nx\nfrom pgmpy.models import BayesianModel\nfrom pgmpy.estimators import HillClimbSearch, BDeuScore, BicScore, BayesianEstimator\nfrom pgmpy.inference import VariableElimination\n\n# Libraries sklearn\nfrom sklearn.model_selection import train_test_split\n\n# Custom Packages\nfrom labimputer.utils.load_dataset import remove_data_outliers\nfrom labimputer.utils.bayes_net import get_unique_edges, get_row_from_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data import \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set relative data path and set FBC panel list\npath_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'\n\nFBC_CODES = [\"EOS\", \"MONO\", \"BASO\", \"NEUT\", \"RBC\", \"WBC\", \n                \"MCHC\", \"MCV\", \"LY\", \"HCT\", \"RDW\", \"HGB\", \n                \"MCH\", \"PLT\", \"MPV\", \"NRBCA\"]\n\n# Read data and drop Nan _uid records\ndf = pd.read_csv(path_data).dropna(subset=['pid'])\n\ndf.reset_index(drop=True, inplace=True)\n\n# Obtain the biomarkers DataFrame only\nraw_data = df[FBC_CODES].dropna(subset=FBC_CODES)\n\n# Remove outliers from dataset\ncomplete_profiles, _ = remove_data_outliers(raw_data)\n\n# Constant variables to drop\nDROP_FEATURES = ['BASO', 'NRBCA']\n\n# Complete profiles for complete case analysis\ncomplete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structure learning\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use the HillClimbSearch to find the best structure of the graph and use a \n# Bayesian Information Criterion (BIC) to set a score for the optimisation \n# problem being solved by HillClimbSearch. For convenience, code has been \n# commented as it takes a couple of minutes (with high CPU requirement) \n# to find the edges but the edges found are consistent for BIC and Bayesian\n# Dirichlet Equivalent Uniform (BDeu).\n\n# hc = HillClimbSearch(complete_df, scoring_method=BicScore(complete_df))\n# best_model = hc.estimate()\n# print(best_model.edges())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model topology \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# edges found from structure learning\nbest_model = [\n    ('EOS', 'LY'), \n    ('MONO', 'WBC'), \n    ('WBC', 'NEUT'), \n    #('MCHC', 'MCV'), \n    ('LY', 'MONO'), \n    ('LY', 'HCT'), \n    ('HCT', 'HGB'), \n    ('HCT', 'RBC'), \n    ('HCT', 'RDW'), \n    #('MCH', 'MCV')\n]\n\n# create an instance of Bayesian Model from pgmpy\nmodel = BayesianModel(best_model)\n\n# get only the variables found for BN\nFEATURES = get_unique_edges(best_model)\n\ncomplete_profiles = complete_profiles[FEATURES]\n\n# get train and test split \n\ntrain, test = train_test_split(complete_profiles, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise BN\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n\nnx.draw(model, node_size=2000, node_color='orange', font_weight='bold', with_labels=True)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter learning\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# estimate the CPDs for each node in the BN\nmodel.fit(train, \n    estimator=BayesianEstimator, \n    prior_type=\"BDeu\")\n\n# show the CPDs\nfor cpd in model.get_cpds():\n    print(f\"\\n {cpd}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate test set data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# test with only top 20 columns and set 10% data to NaN\n\ntest = test[:20]\n\ntcolumns = test.columns\n\nfor col in tcolumns:\n    test.loc[test.sample(frac=0.1).index, col] = np.nan\n\n# show test dataframe with NaN values\n\ntest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference using Variable Elimination\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# use the variable elimination algorithm\nve = VariableElimination(model)\n\n# dataframe to return imputed test data\n\nimp_data = pd.DataFrame()\n\n# compute inference for each row\nfor row in test.itertuples():\n    # get each row in dataframe as a dictionary\n    rowData = get_row_from_df(row, tcolumns)\n\n    # extract the variables with a NaN value\n    nan_vars = [k for k, v in rowData.items() if isnan(v)]\n\n    # extract the dict for observed variables i.e. ones with a value\n    obs_vars = {k: v for k, v in rowData.items() if not isnan(v)}\n\n    # save a copy of the observed variables dict\n    aux = obs_vars.copy()\n\n    # if no observed variables, then leave as is\n\n    if len(nan_vars) > 0:\n\n        # run BP algorithm and obtain the value for \n        # each of the missing variables\n\n        val = ve.map_query(variables=nan_vars,\n                            evidence=obs_vars, \n                            show_progress=False)\n        \n        # add the found values back to aux\n        aux.update(val)\n\n        # add the imputed row data to imp_data DataFrame\n        imp_data = imp_data.append(aux, ignore_index=True)\n\n        # loop again\n        continue\n\n    # otherwise just return the row as it is\n    imp_data = imp_data.append(rowData, ignore_index=True)\n\n# Show imputed test dataframe\n\nimp_data"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}