{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiment 1: Model Learning\n\nThe aim of this experiment was to remove a single feature from the data set \nand use the remaining features to predict its values to emulate a simple \nregression model. This script has results from model learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries import\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Libraries generic\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport networkx as nx\n\n# Libraries sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\nfrom sklearn.model_selection import train_test_split\n\n# Regressors\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\n# Custom Packages\nfrom labimputer.utils.load_dataset import remove_data_outliers\nfrom labimputer.utils.iter_imp import corr_pairs, get_score_statistics, rmse, norm_rmse, rmsle, get_test_scores, nae, get_best_models, get_cvts_delta\nfrom labimputer.core.iter_imp import IterativeImputerRegressor, SimpleImputerRegressor\nfrom labimputer.core.bayes_net import BNRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data import \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set relative data path and set FBC panel list\npath_data = '../resources/datasets/nhs/Transformed_First_FBC_dataset.csv'\n\n# Define FBC panel for the experiment\nFBC_CODES = sorted([\"EOS\", \"MONO\", \"BASO\", \"NEUT\", \"RBC\", \"WBC\", \n                \"MCHC\", \"MCV\", \"LY\", \"HCT\", \"RDW\", \"HGB\", \n                \"MCH\", \"PLT\", \"MPV\", \"NRBCA\"])\n\nRBC_ANALYTES = ['HCT', 'HGB', 'RBC', 'MCH', 'MCV', 'MCHC', 'RDW']\nWBC_ANALYTES = ['EOS', 'MONO', 'LY', 'NEUT', 'WBC']\nPLT_ANALYTES = ['PLT', 'MPV']\n\n# Read data and drop Nan _uid records\ndf = pd.read_csv(path_data).dropna(subset=['pid'])\n\n# Reset the index to easily count all test records\ndf.reset_index(drop=True, inplace=True)\n\n# Obtain the biomarkers DataFrame only\nraw_data = df[FBC_CODES].dropna(subset=FBC_CODES)\n\n# Remove outliers from dataset\ncomplete_profiles, _ = remove_data_outliers(raw_data)\n\n# Constant variables to drop\nDROP_FEATURES = ['BASO', 'NRBCA']\n\n# Complete profiles for complete case analysis\ncomplete_profiles = complete_profiles.drop(DROP_FEATURES, axis=1)\n\nFBC_PANEL = complete_profiles.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define tuned estimators\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_TUNED_ESTIMATORS = {\n    'median': SimpleImputerRegressor(\n        strategy='median'\n    ),\n    'BN': BNRegressor(FBC_PANEL)\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix using Pearson Correlation Coefficient\ncorr_mat = complete_profiles.corr(method='pearson')\n\n# Show\nprint(\"\\nData:\")\nprint(complete_profiles)\nprint(\"\\nCorrelation (pearson):\")\nprint(corr_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split into train-test\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SEED = 8\n\n# Train-test split of 80:20\ntrain_set, test_set = train_test_split(complete_profiles, shuffle=False, test_size=0.2, random_state=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Five fold cross validation (CVTS)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of splits\nn_splits = 5\n\n# Create Kfold instance\nskf = KFold(n_splits=n_splits, shuffle=False)\n\n# Scoring\nscoring = {\n    'nmae': 'neg_mean_absolute_error', # MAE\n    'nmse': 'neg_mean_squared_error',       # MSE\n    'nrmse': 'neg_root_mean_squared_error', # RMSE\n    'rmsle': make_scorer(rmsle), # RMSLE\n    'norm_rmse': make_scorer(norm_rmse), # NRMSE\n}\n\n# Compendium of results\nbn_results = pd.DataFrame()\n\n# Create a list of estimators\nESTIMATORS = [\n    # 'median',\n    # 'BN',\n]\n\n# Loop over each estimator\nfor i, est in enumerate(ESTIMATORS):\n\n    # Dictionary for storing all test scores on hold\n    test_scores = {}\n\n    # Check if estimator has been defined else skip\n    if est not in _TUNED_ESTIMATORS:\n        continue\n    \n    # Select estimator\n    estimator = _TUNED_ESTIMATORS[est]\n    \n    if est != 'median':\n        imputer = IterativeImputerRegressor(estimator=estimator,\n                                            min_value=0, \n                                            max_iter=10000)\n    else:\n        imputer = estimator\n\n    # Loop over each analyte\n    for biomarker in train_set:\n\n        # Generate new train-test for each run\n        aux_train = train_set.copy()\n        aux_test = test_set.copy()\n\n        # Define independent (X_train) and dependent (y_train) variables\n        X_train = aux_train[[x for x in aux_train.columns if x != biomarker]]\n        y_train = aux_train[biomarker]\n\n        # Define same variables with test set\n        X_test = aux_test[[x for x in aux_test.columns if x != biomarker]]\n        y_test = aux_test[biomarker]\n\n        # Information\n        print(\"\\n%s. Evaluating... %s for biomarker... %s\" % (i, est, biomarker))\n\n        # Create pipeline\n        pipe = Pipeline(steps=[ ('dis', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')),\n                                (est, imputer)],\n                        verbose=True)\n\n        # Obtain scores for each fold using cross_validate\n        scores = cross_validate(pipe, \n                                X_train, \n                                y_train, \n                                scoring=scoring, \n                                cv=skf, \n                                return_train_score=True, \n                                n_jobs=-1, \n                                verbose=0)\n\n        # Fit on training set \n        pipe.fit(X_train, y_train)\n\n        # Generate x, y test \n        y_pred = pipe.predict(X_test)\n\n        # Compendium of all test scores\n        test_scores[biomarker] = get_test_scores(y_test, y_pred)\n\n        # Extract results\n        results = pd.DataFrame(scores)\n        results.index = ['%s_%s_%s' % (biomarker, est, j)\n            for j in range(results.shape[0])]\n        \n        # Add to compendium of results\n        bn_results = bn_results.append(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Save\n# bn_results.to_csv('datasets/bn_simple_cv_results.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot BN structure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Make a copy of the training set\naux_train = train_set.copy()\n\n# Discretise data into five bins\ndis = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n\n# Fit transform the discretised data\nXt = pd.DataFrame(dis.fit_transform(aux_train), columns=FBC_PANEL)\n\n# Found from training\nEDGES = [\n    ('HGB', 'RDW'),\n    ('HCT', 'HGB'),\n    ('HCT', 'RBC'),\n    ('MCV', 'RBC'),\n    ('MCV', 'MCHC'),\n    ('RDW', 'LY'),\n    ('RDW', 'MCH'),\n    ('PLT', 'WBC'),\n    ('WBC', 'MONO'),\n    ('WBC', 'NEUT'),\n    ('MCH', 'MCV'),\n    ('MCH', 'MCHC'),\n    ('MPV', 'PLT'),\n    ('LY', 'MONO'),\n    ('LY', 'NEUT'),\n    ('LY', 'WBC'),\n    ('LY', 'PLT'),\n    ('LY', 'EOS'),\n    ('RBC', 'MPV'),\n    ('RBC', 'LY'),\n]\n\n# Initialise the regressor with pre-defined edges (based on previous testing)\nm1 = BNRegressor(FBC_PANEL, EDGES)\n\n# Learn the data using the edges\nm1.fit(Xt)\n\n# Plot\nplt.figure(figsize=(10,8))\n\n# Define colour map for coding\ncolor_map = []\n\nfor node in m1:\n\n    if node in RBC_ANALYTES:\n        color_map.append('salmon')\n    elif node in WBC_ANALYTES:\n        color_map.append('skyblue')\n    else:\n        color_map.append('plum')\n\n# Draw the graph using networkx package\nnx.draw(m1, with_labels=True, \n    node_size = 3000, \n    node_color=color_map, \n    edgecolors='black', \n    font_weight='bold', \n    width=1.5)\n\n# Show\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}